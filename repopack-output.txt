This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-11-08T19:25:14.942Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
.eslintrc.json
.gitignore
.gitpod.yml
.prettierignore
babel.config.ts
cypher/get-history.cypher
cypher/save-response.cypher
examples/chain.mjs
jest.config.js
next.config.js
package.json
postcss.config.js
prompts/agent-scoped.txt
prompts/authoritative-answer-generation.txt
prompts/cypher-evaluation.txt
prompts/cypher-generation-with-instructions.txt
prompts/rephrase-question.txt
prompts/speculative-answer-generation.txt
README.adoc
src/app/globals.css
src/app/layout.tsx
src/app/page.tsx
src/components/form.tsx
src/components/message.tsx
src/components/thinking.tsx
src/hooks/chat.ts
src/modules/agent/agent-scoped.test.ts
src/modules/agent/agent.test.ts
src/modules/agent/agent.ts
src/modules/agent/agent.types.ts
src/modules/agent/chains/answer-generation.chain.ts
src/modules/agent/chains/authoritative-answer-generation.chain.test.ts
src/modules/agent/chains/authoritative-answer-generation.chain.ts
src/modules/agent/chains/rephrase-question.chain.test.ts
src/modules/agent/chains/rephrase-question.chain.ts
src/modules/agent/chains/speculative-answer-generation.chain.test.ts
src/modules/agent/history.test.ts
src/modules/agent/history.ts
src/modules/agent/index.ts
src/modules/agent/tools/cypher/cypher-evaluation.chain.test.ts
src/modules/agent/tools/cypher/cypher-evaluation.chain.ts
src/modules/agent/tools/cypher/cypher-generation.chain.test.ts
src/modules/agent/tools/cypher/cypher-generation.chain.ts
src/modules/agent/tools/cypher/cypher-retrieval.chain.test.ts
src/modules/agent/tools/cypher/cypher-retrieval.chain.ts
src/modules/agent/tools/cypher/cypher-validator.class.ts
src/modules/agent/tools/index.ts
src/modules/agent/tools/tools.test.ts
src/modules/agent/tools/vector-retrieval.chain.test.ts
src/modules/agent/tools/vector-retrieval.chain.ts
src/modules/agent/vector.store.test.ts
src/modules/agent/vector.store.ts
src/modules/graph.test.ts
src/modules/graph.ts
src/modules/llm.ts
src/pages/api/chat.ts
src/solutions/modules/agent/agent.ts
src/solutions/modules/agent/chains/answer-generation.chain.ts
src/solutions/modules/agent/chains/authoritative-answer-generation.chain.ts
src/solutions/modules/agent/chains/rephrase-question.chain.ts
src/solutions/modules/agent/history.ts
src/solutions/modules/agent/index.ts
src/solutions/modules/agent/tools/cypher/cypher-evaluation.chain.ts
src/solutions/modules/agent/tools/cypher/cypher-generation.chain.ts
src/solutions/modules/agent/tools/cypher/cypher-retrieval.chain.ts
src/solutions/modules/agent/tools/index.ts
src/solutions/modules/agent/tools/vector-retrieval.chain.ts
src/solutions/modules/agent/vector.store.ts
src/solutions/modules/graph.ts
src/utils.ts
tailwind.config.js
tsconfig.json

================================================================
Repository Files
================================================================

================
File: .eslintrc.json
================
{
  "extends": "next/core-web-vitals",
  "rules": {
    "indent": ["error", 2],
    "max-len": ["error", 120]
  }
}

================
File: .gitignore
================
# See https://help.github.com/articles/ignoring-files/ for more about ignoring files.

# dependencies
/node_modules
/.pnp
.pnp.js
.yarn/install-state.gz

# testing
/coverage

# next.js
/.next/
/out/

# production
/build

# misc
.DS_Store
*.pem

# debug
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# local env files
.env*.local

# vercel
.vercel

# typescript
*.tsbuildinfo
next-env.d.ts

================
File: .gitpod.yml
================
tasks:
  - init: npm install && npm run build
    command: npm run dev

================
File: .prettierignore
================
*.cypher

================
File: babel.config.ts
================
module.exports = {
    presets: [
      ['@babel/preset-env', {targets: {node: 'current'}}],
      '@babel/preset-typescript',
    ],
  };

================
File: cypher/get-history.cypher
================
MATCH (:Session {id: $sessionId})-[:LAST_RESPONSE]->(last)
// Use string templating to make the limit dynamic: 0..${limit}
MATCH path = (start)-[:NEXT*0..5]->(last)
WHERE length(path) = 5 OR NOT EXISTS { ()-[:NEXT]->(start) }
UNWIND nodes(path) AS response
RETURN response.id AS id,
  response.input AS input,
  response.rephrasedQuestion AS rephrasedQuestion,
  response.output AS output,
  response.cypher AS cypher,
  response.createdAt AS createdAt,
  [ (response)-[:CONTEXT]->(n) | elementId(n) ] AS context

================
File: cypher/save-response.cypher
================
MERGE (session:Session { id: $sessionId }) // <1>

// <2> Create new response
CREATE (response:Response {
  id: randomUuid(),
  createdAt: datetime(),
  source: $source,
  input: $input,
  output: $output,
  rephrasedQuestion: $rephrasedQuestion,
  cypher: $cypher
})
CREATE (session)-[:HAS_RESPONSE]->(response)

WITH session, response

CALL {
  WITH session, response

  // <3> Remove existing :LAST_RESPONSE relationship if it exists
  MATCH (session)-[lrel:LAST_RESPONSE]->(last)
  DELETE lrel

  // <4? Create :NEXT relationship
  CREATE (last)-[:NEXT]->(response)
}

// <5> Create new :LAST_RESPONSE relationship
CREATE (session)-[:LAST_RESPONSE]->(response)

// <6> Create relationship to context nodes
WITH response

CALL {
  WITH response
  UNWIND $ids AS id
  MATCH (context)
  WHERE elementId(context) = id
  CREATE (response)-[:CONTEXT]->(context)

  RETURN count(*) AS count
}

RETURN DISTINCT response.id AS id

================
File: examples/chain.mjs
================
// tag::prompt[]
import { PromptTemplate } from "@langchain/core/prompts";

const prompt = PromptTemplate.fromTemplate(`
You are a cockney fruit and vegetable seller.
Your role is to assist your customer with their fruit and vegetable needs.
Respond using cockney rhyming slang.

Tell me about the following fruit: {fruit}
`);
// end::prompt[]

// tag::llm[]
import { ChatOpenAI } from "@langchain/openai";

const llm = new ChatOpenAI({
  openAIApiKey: "sk-...",
});
// end::llm[]

// tag::parser[]
import { StringOutputParser } from "@langchain/core/output_parsers";

const parser = new StringOutputParser();
// end::parser[]
// tag::passthrough[]
import {
  RunnablePassthrough,
  RunnableSequence,
} from "@langchain/core/runnables";
// end::passthrough[]

/**
 *
 * To ensure type safety, you can define the input and output types

tag::types[]
RunnableSequence.from<InputType, OutputType>
end::types[]

* If you would like the `invoke()` function to accept a string, you can convert
* the input into an Object using a RunnablePassthrough.

// tag::passthrough[]
{
    fruit: new RunnablePassthrough(),
},
// end::passthrough[]
*/

// tag::chain[]
const chain = RunnableSequence.from([prompt, llm, parser]);
// end::chain[]

const main = async () => {
  // tag::invoke[]
  const response = await chain.invoke({ fruit: "pineapple" });

  console.log(response);
  // end::invoke[]
};

main();

================
File: jest.config.js
================
/** @type {import('ts-jest').JestConfigWithTsJest} */
module.exports = {
  preset: "ts-jest",
  testEnvironment: "node",
  setupFiles: ["dotenv/config"],
  testTimeout: 45000,
};

================
File: next.config.js
================
/** @type {import('next').NextConfig} */
const nextConfig = {}

module.exports = nextConfig

================
File: package.json
================
{
  "name": "llm-chatbot-typescript",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "next lint",
    "test": "cross-env DOTENV_CONFIG_PATH=./.env.local jest",
    "test:watch": "cross-env DOTENV_CONFIG_PATH=./.env.local jest --watchAll --detectOpenHandles"
  },
  "dependencies": {
    "@babel/preset-env": "^7.23.8",
    "@langchain/community": "^0.0.30",
    "@langchain/core": "^0.1.48",
    "@langchain/openai": "^0.0.11",
    "@neo4j-cypher/language-support": "^2.0.0-next.3",
    "cross-env": "^7.0.3",
    "dotenv": "^16.4.0",
    "iron-session": "^8.0.1",
    "langchain": "^0.1.28",
    "marked": "^10.0.0",
    "neo4j-driver": "^5.14.0",
    "next": "14.0.2",
    "openai": "^4.20.0",
    "react": "^18",
    "react-dom": "^18"
  },
  "devDependencies": {
    "@babel/preset-typescript": "^7.23.3",
    "@types/jest": "^29.5.11",
    "@types/node": "^20",
    "@types/react": "^18",
    "@types/react-dom": "^18",
    "autoprefixer": "^10.4.16",
    "eslint": "^8",
    "eslint-config-next": "14.0.2",
    "jest": "^29.7.0",
    "postcss": "^8.4.31",
    "tailwindcss": "^3.3.5",
    "ts-jest": "^29.1.2",
    "typescript": "^5.3.3"
  }
}

================
File: postcss.config.js
================
module.exports = {
  plugins: {
    tailwindcss: {},
    autoprefixer: {},
  },
}

================
File: prompts/agent-scoped.txt
================
You are Ebert, a movie recommendation chatbot.
Your goal is to provide movie lovers with excellent recommendations
backed by data from Neo4j, the world's leading graph database.

Respond to any questions that don't relate to movies, actors or directors
with a joke about parrots, before asking them to ask another question
related to the movie industry.

Input: {input}

{agent_scratchpad}

================
File: prompts/authoritative-answer-generation.txt
================
Use the following context to answer the following question.
The context is provided by an authoritative source, you must never doubt
it or attempt to use your pre-trained knowledge to correct the answer.

Make the answer sound like it is a response to the question.
Do not mention that you have based your response on the context.

Here is an example:

Question: Who played Woody in Toy Story?
Context: ['role': 'Woody', 'actor': 'Tom Hanks']
Response: Tom Hanks played Woody in Toy Story.

If no context is provided, say that you don't know,
don't try to make up an answer, and do not fall back on your internal knowledge.
If no context is provided you may also ask for clarification.

Include links and sources where possible.

Question:
{question}

Context:
{context}

================
File: prompts/cypher-evaluation.txt
================
You are an expert Neo4j Developer evaluating a Cypher statement written by an AI.

Check that the cypher statement provided below against the database schema to check that
the statement will answer the user's question.
Fix any errors where possible.

The query must:
* Only use the nodes, relationships and properties mentioned in the schema.
* Assign a variable to nodes or relationships when intending to access their properties.
* Use `IS NOT NULL` to check for property existence.
* Use the `elementId()` function to return the unique identifier for a node or relationship as `_id`.
* For movies, use the tmdbId property to return a source URL.
  For example: `'https://www.themoviedb.org/movie/'+ m.tmdbId AS source`.
* For movie titles that begin with "The", move "the" to the end.
  For example "The 39 Steps" becomes "39 Steps, The" or "the matrix" becomes "Matrix, The".
* For the role a person played in a movie, use the role property on the ACTED_IN relationship.
* Limit the maximum number of results to 10.
* Respond with only a Cypher statement.  No preamble.

Respond with a JSON object with "cypher" and "errors" keys.
  * "cypher" - the corrected cypher statement
  * "corrected" - a boolean
  * "errors" - A list of uncorrectable errors.  For example, if a label,
      relationship type or property does not exist in the schema.
      Provide a hint to the correct element where possible.

Fixable Example #1:
* cypher:
    MATCH (a:Actor {{name: 'Emil Eifrem'}})-[:ACTED_IN]->(m:Movie)
    RETURN a.name AS Actor, m.title AS Movie, m.tmdbId AS source,
    elementId(m) AS _id, m.released AS ReleaseDate, r.role AS Role LIMIT 10
* errors: ["Variable `r` not defined (line 1, column 172 (offset: 171))"]
* response:
    MATCH (a:Actor {{name: 'Emil Eifrem'}})-[r:ACTED_IN]->(m:Movie)
    RETURN a.name AS Actor, m.title AS Movie, m.tmdbId AS source,
    elementId(m) AS _id, m.released AS ReleaseDate, r.role AS Role LIMIT 10


Schema:
{schema}

Question:
{question}

Cypher Statement:
{cypher}

{errors}

================
File: prompts/cypher-generation-with-instructions.txt
================
You are a Neo4j Developer translating user questions into Cypher to answer questions
about movies and provide recommendations.
Convert the user's question into a Cypher statement based on the schema.

You must:
* Only use the nodes, relationships and properties mentioned in the schema.
* When required, `IS NOT NULL` to check for property existence, and not the exists() function.
* Use the `elementId()` function to return the unique identifier for a node or relationship as `_id`.
    For example:
    ```
    MATCH (a:Person)-[:ACTED_IN]->(m:Movie)
    WHERE a.name = 'Emil Eifrem'
    RETURN m.title AS title, elementId(m) AS _id, a.role AS role
    ```
* Include extra information about the nodes that may help an LLM provide a more informative answer,
    for example the release date, rating or budget.
* For movies, use the tmdbId property to return a source URL.
    For example: `'https://www.themoviedb.org/movie/'+ m.tmdbId AS source`.
* For movie titles that begin with "The", move "the" to the end.
    For example "The 39 Steps" becomes "39 Steps, The" or "the matrix" becomes "Matrix, The".
* Limit the maximum number of results to 10.
* Respond with only a Cypher statement.  No preamble.


Example Question: What role did Tom Hanks play in Toy Story?
Example Cypher:
MATCH (a:Actor {{name: 'Tom Hanks'}})-[rel:ACTED_IN]->(m:Movie {{title: 'Toy Story'}})
RETURN a.name AS Actor, m.title AS Movie, elementId(m) AS _id, rel.role AS RoleInMovie

Schema:
{schema}

Question:
{question}

================
File: prompts/rephrase-question.txt
================
Given the following conversation and a question,
rephrase the follow-up question to be a standalone question about the
subject of the conversation history.

If you do not have the required information required to construct
a standalone question, ask for clarification.

Always include the subject of the history in the question.

History:
{history}

Question:
{input}

================
File: prompts/speculative-answer-generation.txt
================
Use only the following context to answer the following question.

Question:
{question}

Context:
{context}

Answer as if you have been asked the original question.
Do not use your pre-trained knowledge.

If you don't know the answer, just say that you don't know, don't try to make up an answer.
Include links and sources where possible.

================
File: README.adoc
================
= Build an Neo4j-backed Chatbot using TypeScript

This repository accompanies the link:https://graphacademy.neo4j.com/courses/llm-chatbot-typescript/?ref=github[Build an Neo4j-backed Chatbot using TypeScript^] course on link:https://graphacademy.neo4j.com/?ref=github[Neo4j GraphAcademy^].

It was originally link:https://nextjs.org/[Next.js] project bootstrapped with link:https://github.com/vercel/next.js/tree/canary/packages/create-next-app[`create-next-app`].  We added:

* link:https://tailwindcss.com/docs/guides/nextjs[TailwindCSS^]
* link:src/pages/api/[A `/api/chat` API route for handling chat requests^]
* link:src/hooks[A React hook for calling the `/api/chat` endpoint^]
* link:src/components[some chat components to display the messages].

For a complete walkthrough of this repository, link:https://graphacademy.neo4j.com/courses/llm-chatbot-typescript/?ref=github[enrol now^].

== Setup your Config

To set config, create a `.env.local` with connection details for your Neo4j Sandbox instance and an OpenAI API Key.
You can also configure the name and description of the chatbot, and the initial greeting message.

[source]
----
NEO4J_URI=bolt://12.34.56.789:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=your-generated-password

OPENAI_API_KEY=sk-...

NEXT_PUBLIC_CHATBOT_NAME=Ebert
NEXT_PUBLIC_CHATBOT_DESCRIPTION="The Movie Recommendation Chatbot"
NEXT_PUBLIC_CHATBOT_GREETING="Hello, I'm **Ebert**, your movie recommendation bot! How can I help you today?"


----


== Running the application

To run the application, you must install the dependencies listed in `package.json`.

[source,sh]
npm i


Then run the `npm run dev` command to start the app on link:http://localhost:3000/[http://localhost:3000/^].

[source,sh]
npm run dev

== Questions, Comments, Feedback

If you have any questions, experience any problems, or have any general feedback, feel free to open an Issue or you can reach out to us on link:https://dev.neo4j.com/chat[Discord] or link:https://dev.neo4j.com/form[Discourse].

================
File: src/app/globals.css
================
@tailwind base;
@tailwind components;
@tailwind utilities;

a {
    --tw-text-opacity: 1;
    color: rgb(29 78 216 / var(--tw-text-opacity));
    font-weight: bold;
    text-decoration: underline;
}

li {
    margin-left: 12px;
    padding-left: 4px;
}
ol li {
    list-style-type:decimal;
}
ul li {
    list-style-type: disc;
}

pre {
    border: 1px solid;
    border-color: rgba(0, 0, 0, .1);;
    background: rgba(0, 0, 0, .02);
    padding: 6px;
    border-radius: 4px;
}

================
File: src/app/layout.tsx
================
import type { Metadata } from 'next'
import './globals.css';

export const metadata: Metadata = {
  title: process.env.NEXT_PUBLIC_CHATBOT_NAME || 'Chatbot Name',
  description: process.env.NEXT_PUBLIC_CHATBOT_DESCRIPTION || 'Chatbot Description',
}

export default function RootLayout({
  children,
}: {
  children: React.ReactNode
}) {
  return (
    <html lang="en">
      <body>
        {children}
      </body>
    </html>
  )
}

================
File: src/app/page.tsx
================
"use client";

import Form from "@/components/form";
import Message from "@/components/message";
import Thinking from "@/components/thinking";
import useChat from "@/hooks/chat";
import React from "react";

export default function Home() {
  const { messages, thinking, container, generateResponse } = useChat();

  const thinkingText = `ðŸ¤” ${
    process.env.NEXT_PUBLIC_CHATBOT_NAME || "Chatbot"
  } is thinking...`;

  return (
    <>
      <div
        className="n- flex n- flex-col n- h-screen n-"
        style={{ height: "100vh" }}
      >
        <div className="p-4  bg-blue-800 flex flex-row justify-between">
          <h1 className="text-white">
            <span className="font-bold">
              {process.env.NEXT_PUBLIC_CHATBOT_NAME || "Chatbot"} -
            </span>
            <span className="text-blue-100">
              {" "}
              {process.env.NEXT_PUBLIC_CHATBOT_DESCRIPTION}
            </span>
          </h1>
        </div>

        <div
          ref={container}
          className="
            flex flex-grow flex-col space-y-4 p-3 overflow-y-auto
            scrollbar-thumb-blue scrollbar-thumb-rounded scrollbar-track-blue-lighter scrollbar-w-2 scrolling-touch"
        >
          {messages.map((m, i) => {
            return <Message key={i} message={m} />;
          })}

          {thinking && <Thinking />}
        </div>

        <Form
          messages={messages}
          thinking={thinking}
          container={container}
          onSubmit={(m) => generateResponse(m)}
        />

        <div className="flex flex-row justify-between b-slate-200 px-4 pb-4 bg-slate-100 text-xs text-slate-600">
          <div className="animate-pulse">{thinking ? thinkingText : " "}</div>
          <div>
            Powered by
            <a href="https://neo4j.com" target="_blank" className="font-bold">
              {" "}
              Neo4j
            </a>{" "}
            &ndash; Learn more at
            <a
              href="https://graphacademy.neo4j.com"
              target="_blank"
              className="font-bold"
            >
              {" "}
              Neo4j GraphAcademy
            </a>
          </div>
        </div>
      </div>
    </>
  );
}

================
File: src/components/form.tsx
================
import { Message } from "@/hooks/chat";
import {
  FormEvent,
  KeyboardEventHandler,
  RefObject,
  useRef,
  useState,
} from "react";

export default function Form({
  onSubmit,
  messages,
  thinking,
  container,
}: {
  onSubmit: (message: string) => void;
  messages: Message[];
  thinking: boolean;
  container: RefObject<HTMLDivElement>;
}) {
  const input = useRef<HTMLTextAreaElement>(null);
  const [message, setMessage] = useState<string>("");

  const handleSubmit = (event?: FormEvent<HTMLFormElement> | SubmitEvent) => {
    event?.preventDefault();

    if (message.trim().length > 0) {
      onSubmit(message);
      setTimeout(() => setMessage(""), 100);

      container.current?.scrollBy(0, 100);
    }
  };

  const handleKeyDown: KeyboardEventHandler<HTMLTextAreaElement> = (e) => {
    if (thinking) {
      return;
    }
    if (e.key === "ArrowUp") {
      const lastHuman = messages.reverse().find((m) => m.role === "human");

      if (lastHuman) {
        setMessage(lastHuman.content as string);
      }
      setTimeout(() => {
        if (input.current) {
          input.current.selectionStart = input.current.value.length;
          input.current.selectionEnd = input.current.value.length;
        }
      }, 20);
    } else if (!e.shiftKey && e.key === "Enter") {
      handleSubmit();
    }
  };

  return (
    <form
      className="border-t b-slate-200 p-4 bg-slate-100"
      onSubmit={(e) => handleSubmit(e)}
    >
      <div className="flex flex-row bg-white border border-slate-600 rounded-md w-full">
        <div className="flex-grow">
          <textarea
            ref={input}
            value={message}
            rows={1}
            className="p-4 border-blue-600 rounded-md w-full outline-none focus:outline-none"
            onChange={(e) => setMessage(e.target.value)}
            onKeyDown={handleKeyDown}
          />
        </div>
        <div className="px-4">
          <button className="px-4 py-4 border-primary-800 text-blue-700 font-bold rounded-md h-full bg-white">
            Send
          </button>
        </div>
      </div>
    </form>
  );
}

================
File: src/components/message.tsx
================
import { parse } from "marked";
import { Message } from "@/hooks/chat";

function fixMarkdown(message: Message): string {
  return parse(message.content).replace(
    '<a href="',
    '<a target="_blank" href="'
  );
}

export default function Message({ message }: { message: Message }) {
  const align = message.role == "ai" ? "justify-start" : "justify-end";
  const no_rounding =
    message.role == "ai" ? "rounded-bl-none" : "rounded-br-none";
  const background = message.role == "ai" ? "blue" : "slate";

  return (
    <div className={`w-full flex flex-row ${align}`}>
      <span className="bg-blue-100"></span>
      <div className="flex flex-col space-y-2 text-sm mx-2 max-w-[60%] order-2 items-start">
        <div className={`bg-${background}-100 p-4 rounded-xl ${no_rounding}`}>
          {/* <div className={`text-${background}-400`}>{message.role}</div> */}

          <div
            dangerouslySetInnerHTML={{
              __html: fixMarkdown(message),
            }}
          />
          {/* <time className={`text-xs font-bold text-${background}-400`}>
            12:32
          </time> */}
        </div>
      </div>
    </div>
  );
}

================
File: src/components/thinking.tsx
================
export default function Thinking() {
  return (
    <div
      id="thinking"
      className="
        inline-flex flex-row justify-center items-center bg-emerald-100 rounded-md w-16
        text-sm mx-2 max-auto p-2 order-2 items-start
      "
    >
      <div className="inline-block w-2 h-2 bg-emerald-800 rounded-full m-1 animate-pulse"></div>
      <div className="inline-block w-2 h-2 bg-emerald-800 rounded-full animate-pulse delay-100"></div>
      <div className="inline-block w-2 h-2 bg-emerald-800 rounded-full m-1 animate-pulse delay-200"></div>
    </div>
  );
}

================
File: src/hooks/chat.ts
================
import { useEffect, useRef, useState } from "react";

export type Message = {
  role: "human" | "ai";
  content: string;
};

export default function useChat() {
  const [thinking, setThinking] = useState<boolean>(false);
  const [messages, setMessages] = useState<Message[]>([
    {
      role: "ai",
      content:
        process.env.NEXT_PUBLIC_CHATBOT_GREETING || "How can I help you today?",
    },
  ]);
  const container = useRef<HTMLDivElement>(null);

  const generateResponse = async (message: string): Promise<void> => {
    // Append human message
    messages.push({ role: "human", content: message });

    // Set thinking to true
    setThinking(true);

    try {
      // Send POST message to the API
      const response = await fetch("/api/chat", {
        method: "POST",
        body: JSON.stringify({ message }),
      });

      // Append the API message to the state
      const json = await response.json();

      messages.push({ role: "ai", content: json.message });

      setMessages(messages);
    } catch (e) {
      console.error(e);
    } finally {
      setThinking(false);
    }
  };

  // Scroll latest message into view
  useEffect(() => {
    if (container.current) {
      container.current.scrollTop = container.current.scrollHeight;
    }
  }, [thinking, messages]);

  return {
    thinking,
    messages,
    container,
    generateResponse,
  };
}

================
File: src/modules/agent/agent-scoped.test.ts
================
import initAgent from "./agent";
import { config } from "dotenv";
import { ChatOpenAI, OpenAIEmbeddings } from "@langchain/openai";
import { Embeddings } from "langchain/embeddings/base";
import { BaseChatModel } from "langchain/chat_models/base";
import { Runnable } from "@langchain/core/runnables";
import { Neo4jGraph } from "@langchain/community/graphs/neo4j_graph";
import { close } from "../graph";

describe("Langchain Agent", () => {
  let llm: BaseChatModel;
  let embeddings: Embeddings;
  let graph: Neo4jGraph;
  let executor: Runnable;

  beforeAll(async () => {
    config({ path: ".env.local" });

    graph = await Neo4jGraph.initialize({
      url: process.env.NEO4J_URI as string,
      username: process.env.NEO4J_USERNAME as string,
      password: process.env.NEO4J_PASSWORD as string,
      database: process.env.NEO4J_DATABASE as string | undefined,
    });

    llm = new ChatOpenAI({
      openAIApiKey: process.env.OPENAI_API_KEY,
      modelName: "gpt-3.5-turbo",
      temperature: 0,
      configuration: {
        baseURL: process.env.OPENAI_API_BASE,
      },
    });

    embeddings = new OpenAIEmbeddings({
      openAIApiKey: process.env.OPENAI_API_KEY as string,
      configuration: {
        baseURL: process.env.OPENAI_API_BASE,
      },
    });

    executor = await initAgent(llm, embeddings, graph);
  });

  afterAll(async () => {
    await graph.close();
    await close();
  });

  describe("Scoping", () => {
    it("should refuse to answer a question not related to movies", async () => {
      const sessionId = "agent-rag-1";
      const input = "Who is the CEO of Neo4j?";

      const output = await executor.invoke(
        {
          input,
        },
        {
          configurable: {
            sessionId,
          },
        }
      );

      expect(output).toContain("ask a question");
    });
  });
});

================
File: src/modules/agent/agent.test.ts
================
import initAgent from "./agent";
import { config } from "dotenv";
import { ChatOpenAI, OpenAIEmbeddings } from "@langchain/openai";
import { Embeddings } from "langchain/embeddings/base";
import { BaseChatModel } from "langchain/chat_models/base";
import { Runnable } from "@langchain/core/runnables";
import { Neo4jGraph } from "@langchain/community/graphs/neo4j_graph";

describe("Langchain Agent", () => {
  let llm: BaseChatModel;
  let embeddings: Embeddings;
  let graph: Neo4jGraph;
  let executor: Runnable;

  beforeAll(async () => {
    config({ path: ".env.local" });

    graph = await Neo4jGraph.initialize({
      url: process.env.NEO4J_URI as string,
      username: process.env.NEO4J_USERNAME as string,
      password: process.env.NEO4J_PASSWORD as string,
      database: process.env.NEO4J_DATABASE as string | undefined,
    });

    llm = new ChatOpenAI({
      openAIApiKey: process.env.OPENAI_API_KEY,
      modelName: "gpt-3.5-turbo",
      temperature: 0,
      configuration: {
        baseURL: process.env.OPENAI_API_BASE,
      },
    });

    embeddings = new OpenAIEmbeddings({
      openAIApiKey: process.env.OPENAI_API_KEY as string,
      configuration: {
        baseURL: process.env.OPENAI_API_BASE,
      },
    });

    executor = await initAgent(llm, embeddings, graph);
  });

  afterAll(() => graph.close());

  describe("Vector Retrieval", () => {
    it("should perform RAG using the neo4j vector retriever", async () => {
      const sessionId = "agent-rag-1";
      const input = "Recommend me a movie about ghosts";

      const output = await executor.invoke(
        {
          input,
        },
        {
          configurable: {
            sessionId,
          },
        }
      );

      // Check database
      const sessionRes = await graph.query(
        `
        MATCH (s:Session {id: $sessionId })-[:LAST_RESPONSE]->(r)
        RETURN r.input AS input, r.output AS output, r.source AS source,
          count { (r)-[:CONTEXT]->() } AS context,
          [ (r)-[:CONTEXT]->(m) | m.title ] AS movies
      `,
        { sessionId }
      );

      expect(sessionRes).toBeDefined();
      if (sessionRes) {
        expect(sessionRes.length).toBe(1);
        expect(sessionRes[0].input).toBe(input);

        let found = false;

        for (const movie of sessionRes[0].movies) {
          if (output.toLowerCase().includes(movie.toLowerCase())) {
            found = true;
          }
        }

        expect(found).toBe(true);
      }
    });
  });
});

================
File: src/modules/agent/agent.ts
================
/* eslint-disable indent */
import { Embeddings } from "@langchain/core/embeddings";
import { Neo4jGraph } from "@langchain/community/graphs/neo4j_graph";
import { ChatPromptTemplate, PromptTemplate } from "@langchain/core/prompts";
import { pull } from "langchain/hub";
import initRephraseChain, {
  RephraseQuestionInput,
} from "./chains/rephrase-question.chain";
import { BaseChatModel } from "langchain/chat_models/base";
import { RunnablePassthrough } from "@langchain/core/runnables";
import { getHistory } from "./history";
import initTools from "./tools";
import { AgentExecutor, createOpenAIFunctionsAgent } from "langchain/agents";

// tag::function[]
export default async function initAgent(
  llm: BaseChatModel,
  embeddings: Embeddings,
  graph: Neo4jGraph
) {
  // TODO: Initiate tools
  // const tools = ...
  // TODO: Pull the prompt from the hub
  // const prompt = ...
  // TODO: Create an agent
  // const agent = ...
  // TODO: Create an agent executor
  // const executor = ...
  // TODO: Create a rephrase question chain
  // const rephraseQuestionChain = ...
  // TODO: Return a runnable passthrough
  // return ...
}
// end::function[]

================
File: src/modules/agent/agent.types.ts
================
import { z } from "zod";

// tag::toolinput[]
export interface ChatAgentInput {
  input: string;
}
// end::toolinput[]

// tag::agenttoolinput[]
export interface AgentToolInput {
  input: string;
  rephrasedQuestion: string;
}
// end::agenttoolinput[]

// tag::schema[]
export const AgentToolInputSchema = z.object({
  input: z.string().describe("The original input sent by the user"),
  rephrasedQuestion: z
    .string()
    .describe(
      "A rephrased version of the original question based on the conversation history"
    ),
});
// end::schema[]

================
File: src/modules/agent/chains/answer-generation.chain.ts
================
import { StringOutputParser } from "@langchain/core/output_parsers";
import { PromptTemplate } from "@langchain/core/prompts";
import { RunnableSequence } from "@langchain/core/runnables";
import { BaseLanguageModel } from "langchain/base_language";
import { OpenAI } from "langchain/llms/openai";

// tag::interface[]
export interface GenerateAnswerInput {
  question: string;
  context: string;
}
// end::interface[]

// tag::function[]
export default function initGenerateAnswerChain(
  llm: BaseLanguageModel
): RunnableSequence<GenerateAnswerInput, string> {
  const answerQuestionPrompt = PromptTemplate.fromTemplate(`
    Use only the following context to answer the following question.

    Question:
    {question}

    Context:
    {context}

    Answer as if you have been asked the original question.
    Do not use your pre-trained knowledge.

    If you don't know the answer, just say that you don't know, don't try to make up an answer.
    Include links and sources where possible.
    `)
  // TODO: Return a RunnableSequence
  return RunnableSequence.from<GenerateAnswerInput, string>([
    answerQuestionPrompt, llm, new StringOutputParser()
  ])
}
// end::function[]


 //* How to use this chain in your application:

// tag::usage[]
const llm = new OpenAI() // Or the LLM of your choice
const answerChain = initGenerateAnswerChain(llm)

const output = await answerChain.invoke({
  question: 'Who is the CEO of Neo4j?',
  context: 'Neo4j CEO: Emil Eifrem',
}) // Emil Eifrem is the CEO of Neo4j
// end::usage[]

================
File: src/modules/agent/chains/authoritative-answer-generation.chain.test.ts
================
import { config } from "dotenv";
import { BaseChatModel } from "langchain/chat_models/base";
import { RunnableSequence } from "@langchain/core/runnables";
import { ChatOpenAI } from "@langchain/openai";
import { PromptTemplate } from "@langchain/core/prompts";
import { StringOutputParser } from "@langchain/core/output_parsers";
import initGenerateAuthoritativeAnswerChain from "./authoritative-answer-generation.chain";

describe("Authoritative Answer Generation Chain", () => {
  let llm: BaseChatModel;
  let chain: RunnableSequence;
  let evalChain: RunnableSequence<any, any>;

  beforeAll(async () => {
    config({ path: ".env.local" });

    llm = new ChatOpenAI({
      openAIApiKey: process.env.OPENAI_API_KEY,
      modelName: "gpt-3.5-turbo",
      temperature: 0,
      configuration: {
        baseURL: process.env.OPENAI_API_BASE,
      },
    });

    chain = await initGenerateAuthoritativeAnswerChain(llm);

    // tag::evalchain[]
    evalChain = RunnableSequence.from([
      PromptTemplate.fromTemplate(`
        Does the following response answer the question provided?

        Question: {question}
        Response: {response}

        Respond simply with "yes" or "no".

        If the response answers the question, reply with "yes".
        If the response does not answer the question, reply with "no".
        If the response asks for more information, reply with "no".
      `),
      llm,
      new StringOutputParser(),
    ]);
    // end::evalchain[]
  });

  describe("Simple RAG", () => {
    it("should use context to answer the question", async () => {
      const question = "Who directed the matrix?";
      const response = await chain.invoke({
        question,
        context: '[{"name": "Lana Wachowski"}, {"name": "Lilly Wachowski"}]',
      });

      // tag::eval[]
      const evaluation = await evalChain.invoke({ question, response });

      expect(`${evaluation.toLowerCase()} - ${response}`).toContain("yes");
      // end::eval[]
    });

    it("should refuse to answer if information is not in context", async () => {
      const question = "Who directed the matrix?";
      const response = await chain.invoke({
        question,
        context: "",
      });

      const evaluation = await evalChain.invoke({ question, response });
      expect(`${evaluation.toLowerCase()} - ${response}`).toContain("no");
    });

    it("should answer this one??", async () => {
      const role = "The Chief";

      const question = "What was Emil Eifrem's role in Neo4j The Movie??";
      const response = await chain.invoke({
        question,
        context: `{"Role":"${role}"}`,
      });

      expect(response).toContain(role);

      const evaluation = await evalChain.invoke({ question, response });
      expect(`${evaluation.toLowerCase()} - ${response}`).toContain("Chief");
    });
  });
});

================
File: src/modules/agent/chains/authoritative-answer-generation.chain.ts
================
import { StringOutputParser } from "@langchain/core/output_parsers";
import { PromptTemplate } from "@langchain/core/prompts";
import {
  RunnablePassthrough,
  RunnableSequence,
} from "@langchain/core/runnables";
import { BaseLanguageModel } from "langchain/base_language";

// tag::interface[]
export type GenerateAuthoritativeAnswerInput = {
  question: string;
  context: string | undefined;
};
// end::interface[]

// tag::function[]
export default function initGenerateAuthoritativeAnswerChain(
  llm: BaseLanguageModel
): RunnableSequence<GenerateAuthoritativeAnswerInput, string> {
  // TODO: Create prompt
  // const answerQuestionPrompt = PromptTemplate.fromTemplate(...)
  // TODO: Return RunnableSequence
  // return RunnableSequence.from(...)
}
// end::function[]

/**
 * How to use this chain in your application:

// tag::usage[]
const llm = new OpenAI() // Or the LLM of your choice
const answerChain = initGenerateAuthoritativeAnswerChain(llm)

const output = await answerChain.invoke({
  input: 'Who is the CEO of Neo4j?',
  context: 'Neo4j CEO: Emil Eifrem',
}) // Emil Eifrem is the CEO of Neo4j
// end::usage[]
 */

================
File: src/modules/agent/chains/rephrase-question.chain.test.ts
================
import { config } from "dotenv";
import { BaseChatModel } from "langchain/chat_models/base";
import { RunnableSequence } from "@langchain/core/runnables";
import { ChatOpenAI } from "@langchain/openai";
import { PromptTemplate } from "@langchain/core/prompts";
import { StringOutputParser } from "@langchain/core/output_parsers";
import initRephraseChain, {
  RephraseQuestionInput,
} from "./rephrase-question.chain";
import { AIMessage, BaseMessage, HumanMessage } from "@langchain/core/messages";
import { ChatbotResponse } from "../history";

describe("Rephrase Question Chain", () => {
  let llm: BaseChatModel;
  let chain: RunnableSequence;
  let evalChain: RunnableSequence<any, any>;

  beforeAll(async () => {
    config({ path: ".env.local" });

    llm = new ChatOpenAI({
      openAIApiKey: process.env.OPENAI_API_KEY,
      modelName: "gpt-3.5-turbo",
      temperature: 0,
      configuration: {
        baseURL: process.env.OPENAI_API_BASE,
      },
    });

    chain = await initRephraseChain(llm);

    evalChain = RunnableSequence.from([
      PromptTemplate.fromTemplate(`
        Is the rephrased version a complete standalone question that can be answered by an LLM?

        Original: {input}
        Rephrased: {response}

        If the question is a suitable standalone question, respond "yes".
        If not, respond with "no".
        If the rephrased question asks for more information, respond with "missing".
      `),
      llm,
      new StringOutputParser(),
    ]);
  });

  describe("Rephrasing Questions", () => {
    it("should handle a question with no history", async () => {
      const input = "Who directed the matrix?";

      const response = await chain.invoke({
        input,
        history: [],
      });

      const evaluation = await evalChain.invoke({ input, response });
      expect(`${evaluation.toLowerCase()} - ${response}`).toContain("yes");
    });

    it("should rephrase a question based on its history", async () => {
      const history = [
        {
          input: "Can you recommend me a film?",
          output: "Sure, I recommend The Matrix",
        },
      ];
      const input = "Who directed it?";
      const response = await chain.invoke({
        input,
        history,
      });

      expect(response).toContain("The Matrix");

      const evaluation = await evalChain.invoke({ input, response });
      expect(`${evaluation.toLowerCase()} - ${response}`).toContain("yes");
    });

    it("should ask for clarification if a question does not make sense", async () => {
      const input = "What about last week?";
      const history: ChatbotResponse[] = [];

      const response = await chain.invoke({
        input,
        history,
      });

      const evaluation = await evalChain.invoke({ input, response });
      expect(`${evaluation.toLowerCase()} - ${response}`).toContain("provide");
    });
  });
});

================
File: src/modules/agent/chains/rephrase-question.chain.ts
================
import { StringOutputParser } from "@langchain/core/output_parsers";
import { PromptTemplate } from "@langchain/core/prompts";
import {
  RunnablePassthrough,
  RunnableSequence,
} from "@langchain/core/runnables";

import { BaseChatModel } from "langchain/chat_models/base";
import { ChatbotResponse } from "../history";

// tag::interface[]
export type RephraseQuestionInput = {
  // The user's question
  input: string;
  // Conversation history of {input, output} from the database
  history: ChatbotResponse[];
}
// end::interface[]

// tag::function[]
export default function initRephraseChain(llm: BaseChatModel) {
  // TODO: Create Prompt template
  // const rephraseQuestionChainPrompt = PromptTemplate.fromTemplate<RephraseQuestionInput, string>(...)
  // TODO: Create Runnable Sequence
  // return RunnableSequence.from<RephraseQuestionInput, string>(
}
// end::function[]

/**
 * How to use this chain in your application:

// tag::usage[]
const llm = new OpenAI() // Or the LLM of your choice
const rephraseAnswerChain = initRephraseChain(llm)

const output = await rephraseAnswerChain.invoke({
  input: 'What else did they act in?',
  history: [{
    input: 'Who played Woody in Toy Story?',
    output: 'Tom Hanks played Woody in Toy Story',
  }]
}) // Other than Toy Story, what movies has Tom Hanks acted in?
// end::usage[]
 */

================
File: src/modules/agent/chains/speculative-answer-generation.chain.test.ts
================
import { config } from "dotenv";
import initGenerateAnswerChain from "./answer-generation.chain";
import { BaseChatModel } from "langchain/chat_models/base";
import { RunnableSequence } from "@langchain/core/runnables";
import { ChatOpenAI } from "@langchain/openai";
import { PromptTemplate } from "@langchain/core/prompts";
import { StringOutputParser } from "@langchain/core/output_parsers";

describe("Speculative Answer Generation Chain", () => {
  let llm: BaseChatModel;
  let chain: RunnableSequence;
  let evalChain: RunnableSequence<any, any>;

  beforeAll(async () => {
    config({ path: ".env.local" });

    llm = new ChatOpenAI({
      openAIApiKey: process.env.OPENAI_API_KEY,
      modelName: "gpt-3.5-turbo",
      temperature: 0,
      configuration: {
        baseURL: process.env.OPENAI_API_BASE,
      },
    });

    chain = await initGenerateAnswerChain(llm);

    // tag::evalchain[]
    evalChain = RunnableSequence.from([
      PromptTemplate.fromTemplate(`
        Does the following response answer the question provided?

        Question: {question}
        Response: {response}

        Respond simply with "yes" or "no".
      `),
      llm,
      new StringOutputParser(),
    ]);
    // end::evalchain[]
  });

  describe("Simple RAG", () => {
    it("should use context to answer the question", async () => {
      const question = "Who directed the matrix?";
      const response = await chain.invoke({
        question,
        context: '[{"name": "Lana Wachowski"}, {"name": "Lilly Wachowski"}]',
      });

      // tag::eval[]
      const evaluation = await evalChain.invoke({ question, response });

      expect(`${evaluation.toLowerCase()} - ${response}`).toContain("yes");
      // end::eval[]
    });

    it("should refuse to answer if information is not in context", async () => {
      const question = "Who directed the matrix?";
      const response = await chain.invoke({
        question,
        context:
          "The Matrix is a 1999 science fiction action film starring Keanu Reeves",
      });

      const evaluation = await evalChain.invoke({ question, response });
      expect(`${evaluation.toLowerCase()} - ${response}`).toContain("no");
    });

    it("should answer this one??", async () => {
      const role = "The Chief";

      const question = "What was Emil Eifrems role in Neo4j The Movie??";
      const response = await chain.invoke({
        question,
        context: `{"Role":"${role}"}`,
      });

      expect(response).toContain(role);

      const evaluation = await evalChain.invoke({ question, response });
      expect(`${evaluation.toLowerCase()} - ${response}`).toContain("yes");
    });
  });
});

================
File: src/modules/agent/history.test.ts
================
import { close } from "../graph";
import { getHistory, saveHistory } from "./history";
import { Neo4jGraph } from "@langchain/community/graphs/neo4j_graph";

describe("Conversation History", () => {
  let graph: Neo4jGraph;
  let ids: string[];

  beforeAll(async () => {
    graph = await Neo4jGraph.initialize({
      url: process.env.NEO4J_URI as string,
      username: process.env.NEO4J_USERNAME as string,
      password: process.env.NEO4J_PASSWORD as string,
    });

    // Delete responses
    await graph.query(
      'MATCH (s:Session)-[:HAS_RESPONSE]->(r:Response) WHERE s.id IN ["test-1", "test-2", "test-3"] DETACH DELETE s, r'
    );

    // Create some test sources to link to
    const [first] = (await graph.query(`
            MERGE (t1:TestSource {id: 1})
            MERGE (t2:TestSource {id: 2})
            MERGE (t3:TestSource {id: 3})

            RETURN [ elementId(t1), elementId(t2), elementId(t3) ] AS ids
        `)) as Record<string, any>[];

    ids = first.ids;
  });

  afterAll(async () => {
    await graph.close();
    await close();
  });

  it("should save conversation history", async () => {
    const sessionId = "test-1";
    const source = "cypher";
    const input = "Who directed The Matrix?";
    const rephrasedQuestion = "Director of The Matrix";
    const output = "The Matrix was directed by The Wachowskis";
    const cypher =
      'MATCH (p:Person)-[:DIRECTED]->(m:Movie {title: "The Matrix"}) RETURN p.name AS name';

    // Save message
    const id = await saveHistory(
      sessionId,
      source,
      input,
      rephrasedQuestion,
      output,
      ids,
      cypher
    );

    expect(id).toBeDefined();

    // Get History
    const history = await getHistory(sessionId, 5);

    expect(history?.length).toBeGreaterThanOrEqual(1);

    const returnedIds = history.map((m) => m.id);

    // Was message returned in the history
    expect(returnedIds).toContain(id);

    // Check sources
    const res = await graph.query(
      `
        MATCH (s:Session {id: $sessionId})-[:LAST_RESPONSE]->(r)
        RETURN r { .* } AS properties,
        [ (r)-[:CONTEXT]->(c) | elementId(c) ] AS context
      `,
      { sessionId }
    );

    expect(res).toBeDefined();
    expect(res?.length).toBeGreaterThanOrEqual(1);

    // Has context been linked?
    const first = res![0];

    expect(first.properties.id).toEqual(id);
    expect(first.properties.source).toEqual(source);
    expect(first.properties.input).toEqual(input);
    expect(first.properties.rephrasedQuestion).toEqual(rephrasedQuestion);
    expect(first.properties.cypher).toEqual(cypher);
    expect(first.properties.output).toEqual(output);

    // Have all context nodes been linked to?
    for (const id of ids) {
      expect(first.context).toContain(id);
    }
  });

  it("should save a chain of responses", async () => {
    const sessionId = "test-3";
    const source = "retriever";
    const firstInput = "Who directed Toy Story?";
    const secondInput = "Who acted in it?";
    const thirdInput = "What else have the acted in together?";

    // Save message
    const messages = [
      await saveHistory(sessionId, source, firstInput, "", "", []),
      await saveHistory(sessionId, source, secondInput, "", "", []),
      await saveHistory(sessionId, source, thirdInput, "", "", []),
    ];

    for (const message of messages) {
      expect(message).toBeDefined();
    }

    // Get History
    const history = await getHistory(sessionId, 5);

    const returnedIds = history.map((m) => m.id).join(",");

    // Responses should be returned in order
    expect(messages.join(",")).toBe(returnedIds);
  });
});

================
File: src/modules/agent/history.ts
================
import { initGraph } from "../graph";

type UnpersistedChatbotResponse = {
  input: string;
  rephrasedQuestion: string;
  output: string;
  cypher: string | undefined;
};

export type ChatbotResponse = UnpersistedChatbotResponse & {
  id: string;
};

// tag::clear[]
export async function clearHistory(sessionId: string): Promise<void> {
  const graph = await initGraph();
  await graph.query(
    `
    MATCH (s:Session {id: $sessionId})-[:HAS_RESPONSE]->(r)
    DETACH DELETE r
  `,
    { sessionId },
    "WRITE"
  );
}
// end::clear[]

// tag::get[]
export async function getHistory(
  sessionId: string,
  limit: number = 5
): Promise<ChatbotResponse[]> {
  // TODO: Execute the Cypher statement from /cypher/get-history.cypher in a read transaction
  // TODO: Use string templating to make the limit dynamic: 0..${limit}
  // const graph = await initGraph()
  // const res = await graph.query<ChatbotResponse>(cypher, { sessionId }, "READ")
  // return res
}
// end::get[]

// tag::save[]
/**
 * Save a question and response to the database
 *
 * @param {string} sessionId
 * @param {string} source
 * @param {string} input
 * @param {string} rephrasedQuestion
 * @param {string} output
 * @param {string[]} ids
 * @param {string | null} cypher
 * @returns {string}  The ID of the Message node
 */
export async function saveHistory(
  sessionId: string,
  source: string,
  input: string,
  rephrasedQuestion: string,
  output: string,
  ids: string[],
  cypher: string | null = null
): Promise<string> {
  // TODO: Execute the Cypher statement from /cypher/save-response.cypher in a write transaction
  // const graph = await initGraph()
  // const res = await graph.query<{id: string}>(cypher, params, "WRITE")
  // return res[0].id
}
// end::save[]

================
File: src/modules/agent/index.ts
================
import { ChatOpenAI } from "@langchain/openai";
import { OpenAIEmbeddings } from "@langchain/openai";
import initAgent from "./agent";
import { initGraph } from "../graph";
import { sleep } from "@/utils";

// tag::call[]
export async function call(input: string, sessionId: string): Promise<string> {
  // TODO: Replace this code with an agent
  await sleep(2000);
  return input;
}
// end::call[]

================
File: src/modules/agent/tools/cypher/cypher-evaluation.chain.test.ts
================
import { ChatOpenAI } from "@langchain/openai";
import { config } from "dotenv";
import { BaseChatModel } from "langchain/chat_models/base";
import { RunnableSequence } from "@langchain/core/runnables";
import { Neo4jGraph } from "@langchain/community/graphs/neo4j_graph";
import initCypherEvaluationChain from "./cypher-evaluation.chain";

describe("Cypher Evaluation Chain", () => {
  let graph: Neo4jGraph;
  let llm: BaseChatModel;
  let chain: RunnableSequence;

  beforeAll(async () => {
    config({ path: ".env.local" });

    graph = await Neo4jGraph.initialize({
      url: process.env.NEO4J_URI as string,
      username: process.env.NEO4J_USERNAME as string,
      password: process.env.NEO4J_PASSWORD as string,
      database: process.env.NEO4J_DATABASE as string | undefined,
    });

    llm = new ChatOpenAI({
      openAIApiKey: process.env.OPENAI_API_KEY,
      modelName: "gpt-3.5-turbo",
      temperature: 0,
      configuration: {
        baseURL: process.env.OPENAI_API_BASE,
      },
    });

    chain = await initCypherEvaluationChain(llm);
  });

  afterAll(async () => {
    await graph.close();
  });

  it("should fix a non-existent label", async () => {
    const input = {
      question: "How many movies are in the database?",
      cypher: "MATCH (m:Muvee) RETURN count(m) AS count",
      schema: graph.getSchema(),
      errors: ["Label Muvee does not exist"],
    };

    const { cypher, errors } = await chain.invoke(input);

    expect(cypher).toContain("MATCH (m:Movie) RETURN count(m) AS count");

    expect(errors.length).toBe(1);

    let found = false;

    for (const error of errors) {
      if (error.includes("label Muvee does not exist")) {
        found = true;
      }
    }

    expect(found).toBe(true);
  });

  it("should fix a non-existent relationship", async () => {
    const input = {
      question: "Who acted in the matrix?",
      cypher:
        'MATCH (m:Muvee)-[:ACTS_IN]->(a:Person) WHERE m.name = "The Matrix" RETURN a.name AS actor',
      schema: graph.getSchema(),
      errors: [
        "Label Muvee does not exist",
        "Relationship type ACTS_IN does not exist",
      ],
    };

    const { cypher, errors } = await chain.invoke(input);

    expect(cypher).toContain("MATCH (m:Movie");
    expect(cypher).toContain(":ACTED_IN");

    expect(errors.length).toBeGreaterThanOrEqual(2);

    let found = false;

    for (const error of errors) {
      if (error.includes("ACTS_IN")) {
        found = true;
      }
    }

    expect(found).toBe(true);
  });

  it("should return no errors if the query is fine", async () => {
    const cypher = "MATCH (m:Movie) RETURN count(m) AS count";
    const input = {
      question: "How many movies are in the database?",
      cypher,
      schema: graph.getSchema(),
      errors: ["Label Muvee does not exist"],
    };

    const { cypher: updatedCypher, errors } = await chain.invoke(input);

    expect(updatedCypher).toContain(cypher);
    expect(errors.length).toBe(0);
  });

  it("should keep variables in relationship", async () => {
    const cypher =
      "MATCH (a:Actor {name: 'Emil Eifrem'})-[r:ACTED_IN]->" +
      "(m:Movie {title: 'Neo4j - Into the Graph'}) RETURN r.role AS Role";
    const input = {
      question: "What role did Emil Eifrem play in Neo4j - Into the Graph",
      cypher,
      schema: graph.getSchema(),
      errors: [],
    };

    const { cypher: updatedCypher, errors } = await chain.invoke(input);

    expect(updatedCypher).toContain(cypher);
    expect(errors.length).toBe(0);
  });
});

================
File: src/modules/agent/tools/cypher/cypher-evaluation.chain.ts
================
import { BaseLanguageModel } from "langchain/base_language";
import { PromptTemplate } from "@langchain/core/prompts";
import {
  RunnablePassthrough,
  RunnableSequence,
} from "@langchain/core/runnables";
import { JsonOutputParser } from "@langchain/core/output_parsers";

// tag::interface[]
export type CypherEvaluationChainInput = {
  question: string;
  cypher: string;
  schema: string;
  errors: string[] | string | undefined;
};
// end::interface[]

// tag::output[]
export type CypherEvaluationChainOutput = {
  cypher: string;
  errors: string[];
};
// end::output[]

// tag::function[]
export default async function initCypherEvaluationChain(
  llm: BaseLanguageModel
) {
  // TODO: Create prompt template
  // const prompt = PromptTemplate.fromTemplate(...)
  // TODO: Return runnable sequence
  // return RunnableSequence.from<input type, output type>(....)
}
// end::function[]

================
File: src/modules/agent/tools/cypher/cypher-generation.chain.test.ts
================
import { ChatOpenAI } from "@langchain/openai";
import { config } from "dotenv";
import { BaseChatModel } from "langchain/chat_models/base";
import { RunnableSequence } from "@langchain/core/runnables";
import { Neo4jGraph } from "@langchain/community/graphs/neo4j_graph";
import initCypherGenerationChain from "./cypher-generation.chain";
import { extractIds } from "../../../../utils";
import { close } from "../../../graph";

describe("Cypher Generation Chain", () => {
  let graph: Neo4jGraph;
  let llm: BaseChatModel;
  let chain: RunnableSequence<string, string>;

  beforeAll(async () => {
    config({ path: ".env.local" });

    graph = await Neo4jGraph.initialize({
      url: process.env.NEO4J_URI as string,
      username: process.env.NEO4J_USERNAME as string,
      password: process.env.NEO4J_PASSWORD as string,
      database: process.env.NEO4J_DATABASE as string | undefined,
    });

    llm = new ChatOpenAI({
      openAIApiKey: process.env.OPENAI_API_KEY,
      modelName: "gpt-3.5-turbo",
      temperature: 0,
      configuration: {
        baseURL: process.env.OPENAI_API_BASE,
      },
    });

    chain = await initCypherGenerationChain(graph, llm);
  });

  afterAll(async () => {
    await graph.close();
    await close();
  });

  it("should generate a simple count query", async () => {
    const output = await chain.invoke("How many movies are in the database?");

    expect(output.toLowerCase()).toContain("match (");
    expect(output).toContain(":Movie");
    expect(output.toLowerCase()).toContain("return");
    expect(output.toLowerCase()).toContain("count(");
  });

  it("should generate a Cypher statement with a relationship", async () => {
    const output = await chain.invoke("Who directed The Matrix?");

    expect(output.toLowerCase()).toContain("match (");
    expect(output).toContain(":Movie");
    expect(output).toContain(":DIRECTED]");
    expect(output.toLowerCase()).toContain("return");
    expect(output.toLowerCase()).toContain("_id");
  });

  it("should extract IDs", () => {
    const ids = extractIds([
      {
        _id: "1",
        name: "Micheal Ward",
        roles: [
          {
            _id: "2",
            name: "Stephen",
            movie: { _id: "3", title: "Empire of Light" },
          },
          {
            _id: "4",
            name: "Marco",
            movie: { _id: "99", title: "Blue Story" },
          },
        ],
      },
      { _id: "100" },
    ]);

    expect(ids).toContain("1");
    expect(ids).toContain("2");
    expect(ids).toContain("3");
    expect(ids).toContain("4");
    expect(ids).toContain("99");
    expect(ids).toContain("100");
  });
});

================
File: src/modules/agent/tools/cypher/cypher-generation.chain.ts
================
import { BaseLanguageModel } from "langchain/base_language";
import { PromptTemplate } from "@langchain/core/prompts";
import {
  RunnablePassthrough,
  RunnableSequence,
} from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers";
import { Neo4jGraph } from "@langchain/community/graphs/neo4j_graph";

// tag::function[]
export default async function initCypherGenerationChain(
  graph: Neo4jGraph,
  llm: BaseLanguageModel
) {
  // TODO: Create Prompt Template
  // const cypherPrompt = PromptTemplate.fromTemplate( ... )
  // TODO: Create the runnable sequence
  // return RunnableSequence.from<string, string>([
}
// end::function[]

================
File: src/modules/agent/tools/cypher/cypher-retrieval.chain.test.ts
================
// TODO: Remove code
import { ChatOpenAI } from "@langchain/openai";
import { config } from "dotenv";
import { BaseChatModel } from "langchain/chat_models/base";
import { Runnable } from "@langchain/core/runnables";
import { Neo4jGraph } from "@langchain/community/graphs/neo4j_graph";
import initCypherRetrievalChain, {
  recursivelyEvaluate,
  getResults,
} from "./cypher-retrieval.chain";
import { close } from "../../../graph";

describe("Cypher QA Chain", () => {
  let graph: Neo4jGraph;
  let llm: BaseChatModel;
  let chain: Runnable;

  beforeAll(async () => {
    config({ path: ".env.local" });

    graph = await Neo4jGraph.initialize({
      url: process.env.NEO4J_URI as string,
      username: process.env.NEO4J_USERNAME as string,
      password: process.env.NEO4J_PASSWORD as string,
      database: process.env.NEO4J_DATABASE as string | undefined,
    });

    llm = new ChatOpenAI({
      openAIApiKey: process.env.OPENAI_API_KEY,
      modelName: "gpt-3.5-turbo",
      temperature: 0,
      configuration: {
        baseURL: process.env.OPENAI_API_BASE,
      },
    });

    chain = await initCypherRetrievalChain(llm, graph);
  });

  afterAll(async () => {
    await graph.close();
    await close();
  });

  it("should answer a simple question", async () => {
    const sessionId = "cypher-retrieval-1";

    const res = (await graph.query(
      `MATCH (n:Movie) RETURN count(n) AS count`
    )) as { count: number }[];

    expect(res).toBeDefined();

    const output = await chain.invoke(
      {
        input: "how many are there?",
        rephrasedQuestion: "How many Movies are in the database?",
      },
      { configurable: { sessionId } }
    );

    expect(output).toContain(res[0].count);
  });

  it("should answer a random question", async () => {
    const sessionId = "cypher-retrieval-2";

    const person = "Emil Eifrem";
    const role = "The Chief";
    const movie = "Neo4j - Into the Graph";

    // Save a fake movie to the database
    await graph.query(
      `
        MERGE (m:Movie {title: $movie})
        MERGE (p:Person {name: $person}) SET p:Actor
        MERGE (p)-[r:ACTED_IN]->(m)
        SET r.role = $role, r.roles = $role
        RETURN
          m { .title, _id: elementId(m) } AS movie,
          p { .name, _id: elementId(p) } AS person
      `,
      { movie, person, role }
    );

    const input = "what did they play?";
    const rephrasedQuestion = `What role did ${person} play in ${movie}`;

    const output = await chain.invoke(
      {
        input,
        rephrasedQuestion,
      },
      { configurable: { sessionId } }
    );

    expect(output).toContain(role);

    // Check persistence
    const contextRes = await graph.query(
      `
      MATCH (s:Session {id: $sessionId})-[:LAST_RESPONSE]->(r)
      RETURN
        r.input AS input,
        r.rephrasedQuestion as rephrasedQuestion,
        r.output AS output,
        [ (m)-[:CONTEXT]->(c) | elementId(c) ] AS ids
    `,
      { sessionId }
    );

    expect(contextRes).toBeDefined();
    if (contextRes) {
      const [first] = contextRes;
      expect(contextRes.length).toBe(1);

      expect(first.input).toEqual(input);
      expect(first.rephrasedQuestion).toEqual(rephrasedQuestion);
      expect(first.output).toEqual(output);
    }
  });

  it("should use elementId() to return a node ID", async () => {
    const sessionId = "cypher-retrieval-3";
    const person = "Emil Eifrem";
    const role = "The Chief";
    const movie = "Neo4j - Into the Graph";

    // Save a fake movie to the database
    const seed = await graph.query(
      `
        MERGE (m:Movie {title: $movie})
        MERGE (p:Person {name: $person}) SET p:Actor
        MERGE (p)-[r:ACTED_IN]->(m)
        SET r.role = $role, r.roles = $role
        RETURN
          m { .title, _id: elementId(m) } AS movie,
          p { .name, _id: elementId(p) } AS person
      `,
      { movie, person, role }
    );

    const output = await chain.invoke(
      {
        input: "what did they play?",
        rephrasedQuestion: `What movies has ${person} acted in?`,
      },
      { configurable: { sessionId } }
    );
    expect(output).toContain(person);
    expect(output).toContain(movie);

    // check context
    const contextRes = await graph.query(
      `
      MATCH (s:Session {id: $sessionId})-[:LAST_RESPONSE]->(r)
      RETURN
        r.input AS input,
        r.rephrasedQuestion as rephrasedQuestion,
        r.output AS output,
        [ (m)-[:CONTEXT]->(c) | elementId(c) ] AS ids
    `,
      { sessionId }
    );

    expect(contextRes).toBeDefined();
    if (contextRes) {
      expect(contextRes.length).toBe(1);

      const contextIds = contextRes[0].ids.join(",");
      const seedIds = seed?.map((el) => el.movie._id);

      for (const id in seedIds) {
        expect(contextIds).toContain(id);
      }
    }
  });

  describe("recursivelyEvaluate", () => {
    it("should correct a query with a missing variable", async () => {
      const res = await recursivelyEvaluate(
        graph,
        llm,
        "What movies has Emil Eifrem acted in?"
      );

      expect(res).toBeDefined();
    });
  });

  describe("getResults", () => {
    it("should fix a broken Cypher statement on the fly", async () => {
      const res = await getResults(graph, llm, {
        question: "What role did Emil Eifrem play in Neo4j - Into the Graph?",
        cypher:
          "MATCH (a:Actor {name: 'Emil Eifrem'})-[:ACTED_IN]->(m:Movie) " +
          "RETURN a.name AS Actor, m.title AS Movie, m.tmdbId AS source, " +
          "elementId(m) AS _id, m.released AS ReleaseDate, r.role AS Role LIMIT 10",
      });

      expect(res).toBeDefined();
      expect(JSON.stringify(res)).toContain("The Chief");
    });
  });
});

================
File: src/modules/agent/tools/cypher/cypher-retrieval.chain.ts
================
import { BaseLanguageModel } from "langchain/base_language";
import { Neo4jGraph } from "@langchain/community/graphs/neo4j_graph";
import { RunnablePassthrough } from "@langchain/core/runnables";
import initCypherGenerationChain from "./cypher-generation.chain";
import initCypherEvaluationChain from "./cypher-evaluation.chain";
import { saveHistory } from "../../history";
import { AgentToolInput } from "../../agent.types";
import { extractIds } from "../../../../utils";
import initGenerateAuthoritativeAnswerChain from "../../chains/authoritative-answer-generation.chain";

// tag::input[]
type CypherRetrievalThroughput = AgentToolInput & {
  context: string;
  output: string;
  cypher: string;
  results: Record<string, any> | Record<string, any>[];
  ids: string[];
};
// end::input[]

// tag::recursive[]
/**
 * Use database the schema to generate and subsequently validate
 * a Cypher statement based on the user question
 *
 * @param {Neo4jGraph}        graph     The graph
 * @param {BaseLanguageModel} llm       An LLM to generate the Cypher
 * @param {string}            question  The rephrased question
 * @returns {string}
 */
export async function recursivelyEvaluate(
  graph: Neo4jGraph,
  llm: BaseLanguageModel,
  question: string
): Promise<string> {
  // TODO: Create Cypher Generation Chain
  // const generationChain = ...
  // TODO: Create Cypher Evaluation Chain
  // const evaluatorChain = ...
  // TODO: Generate Initial cypher
  // let cypher = ...
  // TODO: Recursively evaluate the cypher until there are no errors
  // tag::evaluatereturn[]
  // Bug fix: GPT-4 is adamant that it should use id() regardless of
  // the instructions in the prompt.  As a quick fix, replace it here
  // cypher = cypher.replace(/\sid\(([^)]+)\)/g, " elementId($1)");
  // return cypher;
  // end::evaluatereturn[]
}
// end::recursive[]

// tag::results[]
/**
 * Attempt to get the results, and if there is a syntax error in the Cypher statement,
 * attempt to correct the errors.
 *
 * @param {Neo4jGraph}        graph  The graph instance to get the results from
 * @param {BaseLanguageModel} llm    The LLM to evaluate the Cypher statement if anything goes wrong
 * @param {string}            input  The input built up by the Cypher Retrieval Chain
 * @returns {Promise<Record<string, any>[]>}
 */
export async function getResults(
  graph: Neo4jGraph,
  llm: BaseLanguageModel,
  input: { question: string; cypher: string }
): Promise<any | undefined> {
  // TODO: catch Cypher errors and pass to the Cypher evaluation chain
}
// end::results[]

// tag::function[]
export default async function initCypherRetrievalChain(
  llm: BaseLanguageModel,
  graph: Neo4jGraph
) {
  // TODO: initiate answer chain
  // const answerGeneration = ...
  // TODO: return RunnablePassthrough
}
// end::function[]

================
File: src/modules/agent/tools/cypher/cypher-validator.class.ts
================
import { Neo4jGraph } from "@langchain/community/graphs/neo4j_graph";

export class Relationship {
  constructor(
    public from: string,
    public relationship: string,
    public to: string,
    public properties: Record<string, SchemaProperties>
  ) {}
}

export class Node {
  constructor(
    public label: string,
    public count: number,
    public properties: Record<string, SchemaProperties>
  ) {}
}

export enum RelationshipExistsDecision {
  NOT_FOUND,
  FOUND,
  REVERSE_DIRECTION,
}

export interface SchemaProperties {
  unique: boolean;
  indexed: boolean;
  type: string;
  existence: false;
  array: false;
}

interface SchemaValue {
  count: number;
  labels: string[];
  type: "node" | "relationship";
  properties: Record<string, SchemaProperties>;
  relationships: Record<
    string,
    {
      count: number;
      direction: "out" | "in";
      labels: string[];
      properties: Record<string, SchemaProperties>;
    }
  >;
}

let singleton: CypherValidator;

export class CypherValidator {
  private nodePattern: string = "\\(([^()]*?:[^()]*?)\\)";
  private relationshipPattern: string =
    "\\(([^()]*?:?[^()]*?)\\)(\\<)?-\\[([^\\]]+?)\\]-(\\>)?\\(([^()]*?:?[^()]*?)\\)";

  /* private */
  constructor(
    private readonly graph: Neo4jGraph | null,
    private nodes: Node[] = [],
    private relationships: Relationship[] = []
  ) {}

  /**
   * Reload the schema from the database
   *
   * @returns void
   */
  async reload(): Promise<void> {
    if (!this.graph) {
      return;
    }

    const res = await this.graph.query(`CALL apoc.meta.schema()`);

    if (!res) {
      throw new Error("Could not load schema");
    }

    const [first] = res;
    const rows: Record<string, SchemaValue> = first.value;

    // Build Relationships
    const relationships: Relationship[] = [];
    const nodes: Node[] = [];

    for (const [from, details] of Object.entries(rows)) {
      if (details.type === "node") {
        const node = new Node(from, details.count, details.properties);
        nodes.push(node);

        for (const [type, relDetails] of Object.entries(
          details.relationships
        )) {
          if (relDetails.direction == "out") {
            for (const to of relDetails.labels) {
              relationships.push(
                new Relationship(from, type, to, relDetails.properties)
              );
            }
          }
        }
      }
    }

    this.nodes = nodes;
    this.relationships = relationships;
  }

  /**
   * Get the schema as a string to stuff into a prompt
   *
   * @returns string
   */
  async getSchema(): Promise<string> {
    if (!this.nodes.length || !this.relationships.length) {
      await this.reload();
    }

    const properties = (node: Node | Relationship): string =>
      "{" +
      Object.entries(node.properties)
        .map(([k, v]) => `${k}: ${v.type}`)
        .join(", ") +
      "}";

    const nodes = `Nodes:\n- ${this.nodes
      .map((node) => `(:${node.label} ${properties(node)})`)
      .join("\n- ")}`;
    const relationships = `Relationships:\n- ${this.relationships
      .map(
        (relationship) =>
          `(:${relationship.from})-[:${relationship.relationship} ${properties(
            relationship
          )}]->(:${relationship.to})`
      )
      .join("\n- ")}`;

    return `${nodes}\n\n${relationships}`;
  }

  /**
   * @static
   * Create a new CypherValidator instance and load the schema from the graph
   *
   * @param {Neo4jGraph} graph
   * @returns {Promise<CypherValidator>}
   */
  static async load(graph: Neo4jGraph): Promise<CypherValidator> {
    if (!singleton) {
      singleton = new CypherValidator(graph);
    }

    await singleton.reload();

    return singleton;
  }

  /**
   * Verify that a node label exists in the schema
   *
   * @returns boolean
   */
  private verifyNodeLabel(label: string): boolean {
    return this.nodes.some((node) => node.label == label.trim());
  }

  /**
   * Extract labels from a node pattern
   *
   * @param {string} pattern
   * @returns {string[]}
   */
  private extractLabels(pattern: string | undefined): string[] {
    // Handle anonymous or node variables
    if (pattern === undefined || !pattern.includes(":")) {
      return [""];
    }

    // Strip brackets
    if (pattern.endsWith(")")) {
      pattern = pattern.substring(0, pattern.length - 1);
    }
    if (pattern.startsWith("(")) {
      pattern = pattern.substring(1);
    }

    if (pattern.includes("{")) {
      pattern = pattern.split("{")[0];
    }

    // Split labels
    if (pattern.includes(":")) {
      const labels = pattern.split(":");

      // Remove variable or empty string
      labels.splice(0, 1);

      return labels.map((label) => label.trim());
    }

    return [pattern];
  }

  /**
   * Extract relationship types from a relationship pattern
   *
   * @param {string} pattern
   * @returns {string[]}
   */
  private extractRelationshipTypes(pattern: string): string[] {
    let cleaned = pattern;

    // Strip brackets
    if (cleaned.includes("[")) {
      cleaned = cleaned.split("[")[1];
    }

    if (cleaned.includes("]")) {
      cleaned = cleaned.split("]")[0];
    }

    // Strip properties
    if (cleaned.includes("{")) {
      cleaned = cleaned.split("{")[0];
    }

    //  Strip variable
    if (cleaned.includes(":")) {
      const parts = cleaned.split(":");
      cleaned = parts[parts.length - 1];
    }
    // Strip variable length path
    if (cleaned.includes("*")) {
      const parts = cleaned.split("*");
      cleaned = parts[0];
    }

    return cleaned.split("|");
  }

  /**
   * Determine if any relationship exists between the given node labels.
   * If the relationship does not exist, check if the reverse direction exists
   * and if so, return the instruction to reverse the relationship.
   *
   * @param {string[]} from
   * @param {string[]} rel
   * @param {string[]} to
   * @returns {RelationshipExistsDecision}
   */
  private anyRelationshipExists(
    from: string[],
    rel: string[],
    to: string[]
  ): RelationshipExistsDecision {
    for (const f of from) {
      for (const t of to) {
        for (const r of rel) {
          if (this.relationshipExists(f, r, t)) {
            return RelationshipExistsDecision.FOUND;
          } else if (this.relationshipExists(t, r, f)) {
            return RelationshipExistsDecision.REVERSE_DIRECTION;
          }
        }
      }
    }
    return RelationshipExistsDecision.NOT_FOUND;
  }

  /**
   * Determine whether any of the relationship types exist
   *
   * @param {string[]} rels
   * @returns {boolean}
   */
  private anyRelationshipTypeExists(rels: string[]): boolean {
    for (const rel of rels) {
      if (this.relationshipTypeExists(rel)) {
        return true;
      }
    }
    return false;
  }

  /**
   * Determine whether the relationship type exists in the database
   *
   * @param {string} rel
   * @returns {boolean}
   */
  private relationshipTypeExists(rel: string): boolean {
    return this.relationships.some((schema) => schema.relationship == rel);
  }

  /**
   * Determine whether the relationship exists in the database
   *
   * @param {string} from
   * @param {string} rel
   * @param {string} to
   * @returns {boolean}
   */
  private relationshipExists(from: string, rel: string, to: string): boolean {
    if (from === "") {
      return this.relationships.some(
        (schema) => schema.relationship == rel && schema.to == to
      );
    } else if (to === "") {
      return this.relationships.some(
        (schema) => schema.relationship == rel && schema.from == from
      );
    } else {
      return this.relationships.some(
        (schema) =>
          schema.relationship == rel && schema.from == from && schema.to == to
      );
    }
  }

  /**
   * Helper function to consistently format the error message
   * when a node label is not found in the schema
   *
   * @param {string} label
   * @returns {string}
   */
  private noLabelError(label: string): string {
    return `Node label not found: ${label}`;
  }

  /**
   * Helper function to consistently format the error message
   * when the relationship types are not found in the schema
   *
   * @param {string[]} types
   * @returns {string}
   */
  private noRelationshipTypeError(types: string[]): string {
    return `Relationship type(s) not found: ${types.join("|")}`;
  }

  /**
   * Helper function to consistently format the error message
   * when the relationship type does not exist between the two nodes
   *
   * @param {string[]} types
   * @returns {string}
   */
  private noRelationshipError(
    from: string[],
    rel: string[],
    to: string[]
  ): string {
    return `Relationship combination not found: (:${from.join(
      ":"
    )})-[:${rel.join("|")}]->(:${to.join(":")})`;
  }

  /**
   * Given a query string, validate the query and return the query string and any errors.
   * If a relationship is written in the wrong direction, this function will correct it.
   * If any nodes or patterns do not exist, the details will be returned in the errors array.
   *
   * @param {string} query
   * @returns {{ query: string, errors: string[] }}
   */
  validate(query: string): { query: string; errors: string[] } {
    // Given a query string: MATCH (a:Person)-[:ACTED_IN]->(b:Movie) RETURN a, b)
    // Extract the pattern: (a:Person)-[:ACTED_IN]->(b:Movie)
    const errors = [];

    // Verify labels
    const nodePattern = new RegExp(`${this.nodePattern}`, "g");
    for (const node of query.matchAll(nodePattern)) {
      const labels = this.extractLabels(node[1]);

      for (const label of labels) {
        if (
          !label.includes(".") &&
          label.trim() !== "" &&
          !this.verifyNodeLabel(label)
        ) {
          errors.push(this.noLabelError(label));
        }
      }
    }

    const patternRegex = new RegExp(this.relationshipPattern, "g");
    const matches = query.matchAll(patternRegex);

    for (const match of matches) {
      const [pattern, left, incoming, rel, outgoing, right] = match;

      const leftLabels = this.extractLabels(left);
      const rightLabels = this.extractLabels(right);
      const relationshipTypes = this.extractRelationshipTypes(rel);

      if (!this.anyRelationshipTypeExists(relationshipTypes)) {
        errors.push(this.noRelationshipTypeError(relationshipTypes));
      }
      // - If direction is OUTGOING, find schema items where
      // `from` is the same as the first node label and `relationship`
      //  is the same as the relationship type
      else if (outgoing !== undefined) {
        const exists = this.anyRelationshipExists(
          leftLabels,
          relationshipTypes,
          rightLabels
        );

        if (exists === RelationshipExistsDecision.NOT_FOUND) {
          errors.push(
            this.noRelationshipError(leftLabels, relationshipTypes, rightLabels)
          );
        } else if (exists === RelationshipExistsDecision.REVERSE_DIRECTION) {
          query = query.replace(pattern, `(${left})<-[${rel}]-(${right})`);
        }
      }
      // - if direction is incomingm find schema items where `to` is the
      // same as the first node label and `relationship` is the same
      // as the relationship type
      else if (incoming !== undefined) {
        const exists = this.anyRelationshipExists(
          rightLabels,
          relationshipTypes,
          leftLabels
        );

        if (exists === RelationshipExistsDecision.NOT_FOUND) {
          errors.push(
            this.noRelationshipError(rightLabels, relationshipTypes, leftLabels)
          );
        } else if (exists === RelationshipExistsDecision.REVERSE_DIRECTION) {
          query = query.replace(pattern, `(${left})-[${rel}]->(${right})`);
        }
      }
    }

    return { query, errors };
  }

  call(query: string): string {
    const { query: correctedQuery, errors } = this.validate(query);

    if (errors.length > 0) {
      return `Your query: \n${query} has the following errors: \n${errors.join(
        "\n"
      )} `;
    }

    return correctedQuery;
  }
}

================
File: src/modules/agent/tools/index.ts
================
import { BaseChatModel } from "langchain/chat_models/base";
import { Embeddings } from "@langchain/core/embeddings";
import { Neo4jGraph } from "@langchain/community/graphs/neo4j_graph";
import initCypherRetrievalChain from "./cypher/cypher-retrieval.chain";
import initVectorRetrievalChain from "./vector-retrieval.chain";
import { DynamicStructuredTool } from "@langchain/community/tools/dynamic";
import { AgentToolInputSchema } from "../agent.types";
import { RunnableConfig } from "langchain/runnables";

// tag::function[]
export default async function initTools(
  llm: BaseChatModel,
  embeddings: Embeddings,
  graph: Neo4jGraph
): Promise<DynamicStructuredTool[]> {
  // TODO: Initiate chains
  // const cypherChain = await ...
  // const retrievalChain = await ...

  // TODO: Append chains to output
  return [];
}
// end::function[]

================
File: src/modules/agent/tools/tools.test.ts
================
import { ChatOpenAI } from "@langchain/openai";
import initTools from ".";
import { Neo4jGraph } from "@langchain/community/graphs/neo4j_graph";
import { OpenAIEmbeddings } from "@langchain/openai";

describe("Tool Chain", () => {
  it("should return two tools", async () => {
    const graph = await Neo4jGraph.initialize({
      url: process.env.NEO4J_URI as string,
      username: process.env.NEO4J_USERNAME as string,
      password: process.env.NEO4J_PASSWORD as string,
      database: process.env.NEO4J_DATABASE as string | undefined,
    });

    const llm = new ChatOpenAI({
      openAIApiKey: process.env.OPENAI_API_KEY,
      modelName: "gpt-3.5-turbo",
      temperature: 0,
      configuration: {
        baseURL: process.env.OPENAI_API_BASE,
      },
    });

    const embeddings = new OpenAIEmbeddings({
      openAIApiKey: process.env.OPENAI_API_KEY as string,
      configuration: {
        baseURL: process.env.OPENAI_API_BASE,
      },
    });

    const tools = await initTools(llm, embeddings, graph);

    expect(tools).toBeDefined();
    expect(tools.length).toBeGreaterThanOrEqual(2);

    await graph.close();
  });
});

================
File: src/modules/agent/tools/vector-retrieval.chain.test.ts
================
import { ChatOpenAI, OpenAIEmbeddings } from "@langchain/openai";
import { config } from "dotenv";
import { BaseChatModel } from "langchain/chat_models/base";
import { Embeddings } from "langchain/embeddings/base";
import { Runnable } from "@langchain/core/runnables";
import initVectorRetrievalChain from "./vector-retrieval.chain";
import { Neo4jGraph } from "@langchain/community/graphs/neo4j_graph";
import { AgentToolInput } from "../agent.types";
import { close } from "../../graph";

describe("Vector Retrieval Chain", () => {
  let graph: Neo4jGraph;
  let llm: BaseChatModel;
  let embeddings: Embeddings;
  let chain: Runnable<AgentToolInput, string>;

  beforeAll(async () => {
    config({ path: ".env.local" });

    graph = await Neo4jGraph.initialize({
      url: process.env.NEO4J_URI as string,
      username: process.env.NEO4J_USERNAME as string,
      password: process.env.NEO4J_PASSWORD as string,
      database: process.env.NEO4J_DATABASE as string | undefined,
    });

    llm = new ChatOpenAI({
      openAIApiKey: process.env.OPENAI_API_KEY,
      modelName: "gpt-3.5-turbo",
      temperature: 0,
      configuration: {
        baseURL: process.env.OPENAI_API_BASE,
      },
    });

    embeddings = new OpenAIEmbeddings({
      openAIApiKey: process.env.OPENAI_API_KEY as string,
      configuration: {
        baseURL: process.env.OPENAI_API_BASE,
      },
    });

    chain = await initVectorRetrievalChain(llm, embeddings);
  });

  afterAll(async () => {
    await graph.close();
    await close();
  });

  it("should provide a recommendation", async () => {
    const sessionId = "vector-retriever-1";
    const input = "[redacted]";
    const rephrasedQuestion = "Recommend a movie about ghosts";

    const output = await chain.invoke(
      {
        input,
        rephrasedQuestion,
      },
      { configurable: { sessionId } }
    );

    // Should generate an answer
    expect(output).toBeDefined();

    // Should save to the database
    const res = await graph.query(
      `
        MATCH (s:Session {id: $sessionId})-[:LAST_RESPONSE]->(r)
        RETURN s.id AS session, r.input AS input, r.output AS output,
          r.rephrasedQuestion AS rephrasedQuestion,
          [ (r)-[:CONTEXT]->(m) | m.title ] AS context
        ORDER BY r.createdAt DESC LIMIT 1
    `,
      { sessionId }
    );

    expect(res).toBeDefined();

    // Should have properties set
    const [first] = res!;

    expect(first.input).toEqual(input);
    expect(first.rephrasedQuestion).toEqual(rephrasedQuestion);
    expect(first.output).toEqual(output);
    expect(first.input).toEqual(input);

    // Should save with context
    expect(first.context.length).toBeGreaterThanOrEqual(1);

    // Any of the movies in the context should be mentioned
    let found = false;

    for (const title of first.context) {
      if (output.includes(title.replace(", The", ""))) {
        found = true;
      }
    }

    expect(found).toBe(true);
  });
});

================
File: src/modules/agent/tools/vector-retrieval.chain.ts
================
import {
  Runnable,
  RunnablePassthrough,
  RunnablePick,
} from "@langchain/core/runnables";
import { Embeddings } from "@langchain/core/embeddings";
import initGenerateAnswerChain from "../chains/answer-generation.chain";
import { BaseLanguageModel } from "langchain/base_language";
import initVectorStore from "../vector.store";
import { saveHistory } from "../history";
import { DocumentInterface } from "@langchain/core/documents";
import { AgentToolInput } from "../agent.types";

// tag::throughput[]
type RetrievalChainThroughput = AgentToolInput & {
  context: string;
  output: string;
  ids: string[];
};
// end::throughput[]

// tag::extractDocumentIds[]
// Helper function to extract document IDs from Movie node metadata
const extractDocumentIds = (
  documents: DocumentInterface<{ _id: string; [key: string]: any }>[]
): string[] => documents.map((document) => document.metadata._id);
// end::extractDocumentIds[]

// tag::docsToJson[]
// Convert documents to string to be included in the prompt
const docsToJson = (documents: DocumentInterface[]) =>
  JSON.stringify(documents);
// end::docsToJson[]

// tag::function[]
export default async function initVectorRetrievalChain(
  llm: BaseLanguageModel,
  embeddings: Embeddings
): Promise<Runnable<AgentToolInput, string>> {
  // TODO: Create vector store instance
  // const vectorStore = ...
  // TODO: Initialize a retriever wrapper around the vector store
  // const vectorStoreRetriever = ...
  // TODO: Initialize Answer chain
  // const answerChain = ...
  // TODO: Return chain
  // return RunnablePassthrough.assign( ... )
}
// end::function[]

================
File: src/modules/agent/vector.store.test.ts
================
import { OpenAIEmbeddings } from "@langchain/openai";
import initVectorStore from "./vector.store";
import { Neo4jVectorStore } from "@langchain/community/vectorstores/neo4j_vector";
import { close } from "../graph";

describe("Vector Store", () => {
  afterAll(() => close());

  it("should instantiate a new vector store", async () => {
    const embeddings = new OpenAIEmbeddings({
      openAIApiKey: process.env.OPENAI_API_KEY as string,
      configuration: {
        baseURL: process.env.OPENAI_API_BASE,
      },
    });
    const vectorStore = await initVectorStore(embeddings);
    expect(vectorStore).toBeInstanceOf(Neo4jVectorStore);

    await vectorStore.close();
  });

  it("should create a test index", async () => {
    const indexName = "test-index";
    const embeddings = new OpenAIEmbeddings({
      openAIApiKey: process.env.OPENAI_API_KEY as string,
      configuration: {
        baseURL: process.env.OPENAI_API_BASE,
      },
    });

    const index = await Neo4jVectorStore.fromTexts(
      ["Neo4j GraphAcademy offers free, self-paced online training"],
      [],
      embeddings,
      {
        url: process.env.NEO4J_URI as string,
        username: process.env.NEO4J_USERNAME as string,
        password: process.env.NEO4J_PASSWORD as string,
        nodeLabel: "Test",
        embeddingNodeProperty: "embedding",
        textNodeProperty: "text",
        indexName,
      }
    );

    expect(index).toBeInstanceOf(Neo4jVectorStore);
    expect(index["indexName"]).toBe(indexName);

    await index.close();
  });
});

================
File: src/modules/agent/vector.store.ts
================
import { EmbeddingsInterface } from "@langchain/core/embeddings";
import { Neo4jVectorStore } from "@langchain/community/vectorstores/neo4j_vector";

/**
 * Create a new vector search index that uses the existing
 * `moviePlots` index.
 *
 * @param {EmbeddingsInterface} embeddings  The embeddings model
 * @returns {Promise<Neo4jVectorStore>}
 */
// tag::function[]
export default async function initVectorStore(
  embeddings: EmbeddingsInterface
): Promise<Neo4jVectorStore> {
  // TODO: Create vector store
  // const vectorStore = await Neo4jVectorStore.fromExistingIndex(embeddings, { ... })
  // return vectorStore
}
// end::function[]

================
File: src/modules/graph.test.ts
================
import { initGraph } from "./graph";
import { Neo4jGraph } from "@langchain/community/graphs/neo4j_graph";

describe("Neo4j Graph", () => {
  it("should have environment variables defined", () => {
    expect(process.env.NEO4J_URI).toBeDefined();
    expect(process.env.NEO4J_USERNAME).toBeDefined();
    expect(process.env.NEO4J_PASSWORD).toBeDefined();
  });

  describe("initGraph", () => {
    it("should instantiate Neo4jGraph", async () => {
      const graph = await initGraph();

      expect(graph).toBeInstanceOf(Neo4jGraph);

      await graph.query("MERGE (t:DriverTest {working: true})");

      await graph.close();
    });
  });
});

================
File: src/modules/graph.ts
================
// tag::import[]
import { Neo4jGraph } from "@langchain/community/graphs/neo4j_graph";
// end::import[]

// tag::graph[]
// <1> The singleton instance
let graph: Neo4jGraph;

/**
 * <2> Return the existing `graph` object or create one
 * has not already been created
 * @returns {Promise<Neo4jGraph>}
 */
export async function initGraph(): Promise<Neo4jGraph> {
  if (!graph) {
    // TODO: Create singleton and wait for connection to be verified
  }

  return graph;
}
// end::graph[]

/**
 * Close any connections to Neo4j initiated in this file.
 *
 * @returns {Promise<void>}
 */
export async function close(): Promise<void> {
  if (graph) {
    await graph.close();
  }
}

================
File: src/modules/llm.ts
================
import { ChatOpenAI, OpenAIEmbeddings } from "@langchain/openai";

export const llm = new ChatOpenAI({
  openAIApiKey: process.env.OPENAI_API_KEY,
  modelName: "gpt-4",
  temperature: 0,
});

export const embeddings = new OpenAIEmbeddings({
  openAIApiKey: process.env.OPENAI_API_KEY as string,
});

================
File: src/pages/api/chat.ts
================
import { call } from "@/modules/agent";
import { randomUUID } from "crypto";
import type { NextApiRequest, NextApiResponse } from "next";

type ResponseData = {
  message: string;
};

function getSessionId(req: NextApiRequest, res: NextApiResponse): string {
  let sessionId: string | undefined = req.cookies["session"];

  if (typeof sessionId === "string") {
    return sessionId;
  }

  // Assign a new session
  sessionId = randomUUID();
  res.setHeader("Set-Cookie", `session=${sessionId}`);

  return sessionId;
}

export default async function handler(
  req: NextApiRequest,
  res: NextApiResponse<ResponseData>
) {
  if (req.method === "POST") {
    const body = JSON.parse(req.body);
    const message = body.message;

    // Get or assign the Session ID
    const sessionId = getSessionId(req, res);

    try {
      const result = await call(message, sessionId);

      res.status(201).json({
        message: result,
      });
    } catch (e: any) {
      res.status(500).json({
        message: `I'm suffering from brain fog...\n\n${e.message}`,
      });
    }
  } else {
    res.status(404).send({ message: "Route not found" });
  }
}

================
File: src/solutions/modules/agent/agent.ts
================
/* eslint-disable indent */
import { Embeddings } from "@langchain/core/embeddings";
import { Neo4jGraph } from "@langchain/community/graphs/neo4j_graph";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { AgentExecutor, createOpenAIFunctionsAgent } from "langchain/agents";
import { pull } from "langchain/hub";
import initRephraseChain, {
  RephraseQuestionInput,
} from "./chains/rephrase-question.chain";
import { BaseChatModel } from "langchain/chat_models/base";
import { RunnablePassthrough } from "@langchain/core/runnables";
import { getHistory } from "./history";
import initTools from "./tools";

/**
 * To restrict the scope of the agent, you can issue specific instructions
 * in the prompt.
 *
 * The {agent_scratchpad} placeholder is used by the prompt to store any
 * _thinking_ that the agent has performed while selecting the appropriate tool
 */
// tag::scoped[]
import { MessagesPlaceholder } from "@langchain/core/prompts";

const prompt = ChatPromptTemplate.fromMessages<{
  'chat_history': string, 
  agent_scratchpad: string, 
  rephrasedQuestion: string
}>([
  ['system', `
  You are Ebert, a movie recommendation chatbot.
  Your goal is to provide movie lovers with excellent recommendations
  backed by data from Neo4j, the world's leading graph database.

  Respond to any questions that don't relate to movies, actors or directors
  with a joke about parrots, before asking them to ask another question
  related to the movie industry.
  `,
  ],
  [ 'human', '{rephrasedQuestion}' ],
  new MessagesPlaceholder({variableName: 'chat_history', optional: true}),
  new MessagesPlaceholder('agent_scratchpad'),
]);
// end::scoped[]

// tag::function[]
export default async function initAgent(
  llm: BaseChatModel,
  embeddings: Embeddings,
  graph: Neo4jGraph
) {
  // tag::tools[]
  const tools = await initTools(llm, embeddings, graph);
  // end::tools[]

  // tag::prompt[]
  const prompt = await pull<ChatPromptTemplate>(
    "hwchase17/openai-functions-agent"
  );
  // end::prompt[]

  // tag::agent[]
  const agent = await createOpenAIFunctionsAgent({
    llm,
    tools,
    prompt,
  });
  // end::agent[]

  // tag::executor[]
  const executor = new AgentExecutor({
    agent,
    tools,
    verbose: true, // Verbose output logs the agents _thinking_
  });
  // end::executor[]

  // tag::rephrasechain[]
  const rephraseQuestionChain = await initRephraseChain(llm);
  // end::rephrasechain[]

  // tag::history[]
  return (
    RunnablePassthrough.assign<{ input: string; sessionId: string }, any>({
      // Get Message History
      history: async (_input, options) => {
        const history = await getHistory(
          options?.config.configurable.sessionId
        );

        return history;
      },
    })
      // end::history[]
      // tag::rephrase[]
      .assign({
        // Use History to rephrase the question
        rephrasedQuestion: (input: RephraseQuestionInput, config: any) =>
          rephraseQuestionChain.invoke(input, config),
      })
      // end::rephrase[]

      // tag::execute[]
      // Pass to the executor
      .pipe(executor)
      // end::execute[]
      // tag::output[]
      .pick("output")
  );
  // end::output[]
}
// end::function[]

================
File: src/solutions/modules/agent/chains/answer-generation.chain.ts
================
import { StringOutputParser } from "@langchain/core/output_parsers";
import { PromptTemplate } from "@langchain/core/prompts";
import { RunnableSequence } from "@langchain/core/runnables";
import { BaseLanguageModel } from "langchain/base_language";

// tag::interface[]
export interface GenerateAnswerInput {
  question: string;
  context: string;
}
// end::interface[]

// tag::function[]
export default function initGenerateAnswerChain(
  llm: BaseLanguageModel
): RunnableSequence<GenerateAnswerInput, string> {
  // tag::prompt[]
  const answerQuestionPrompt = PromptTemplate.fromTemplate(`
    Use only the following context to answer the following question.

    Question:
    {question}

    Context:
    {context}

    Answer as if you have been asked the original question.
    Do not use your pre-trained knowledge.

    If you don't know the answer, just say that you don't know, don't try to make up an answer.
    Include links and sources where possible.
  `);
  // end::prompt[]

  // tag::sequence[]
  return RunnableSequence.from<GenerateAnswerInput, string>([
    answerQuestionPrompt,
    llm,
    new StringOutputParser(),
  ]);
  // end::sequence[]
}
// end::function[]

/**
 * How to use this chain in your application:

// tag::usage[]
const llm = new OpenAI() // Or the LLM of your choice
const answerChain = initGenerateAnswerChain(llm)

const output = await answerChain.invoke({
  input: 'Who is the CEO of Neo4j?',
  context: 'Neo4j CEO: Emil Eifrem',
}) // Emil Eifrem is the CEO of Neo4j
// end::usage[]
 */

================
File: src/solutions/modules/agent/chains/authoritative-answer-generation.chain.ts
================
import { StringOutputParser } from "@langchain/core/output_parsers";
import { PromptTemplate } from "@langchain/core/prompts";
import {
  RunnablePassthrough,
  RunnableSequence,
} from "@langchain/core/runnables";
import { BaseLanguageModel } from "langchain/base_language";

// tag::interface[]
export type GenerateAuthoritativeAnswerInput = {
  question: string;
  context: string | undefined;
};
// end::interface[]

export default function initGenerateAuthoritativeAnswerChain(
  llm: BaseLanguageModel
): RunnableSequence<GenerateAuthoritativeAnswerInput, string> {
  // tag::prompt[]
  const answerQuestionPrompt = PromptTemplate.fromTemplate(`
    Use the following context to answer the following question.
    The context is provided by an authoritative source, you must never doubt
    it or attempt to use your pre-trained knowledge to correct the answer.

    Make the answer sound like it is a response to the question.
    Do not mention that you have based your response on the context.

    Here is an example:

    Question: Who played Woody in Toy Story?
    Context: ['role': 'Woody', 'actor': 'Tom Hanks']
    Response: Tom Hanks played Woody in Toy Story.

    If no context is provided, say that you don't know,
    don't try to make up an answer, do not fall back to your internal knowledge.
    If no context is provided you may also ask for clarification.

    Include links and sources where possible.

    Question:
    {question}

    Context:
    {context}
  `);
  // end::prompt[]

  // tag::sequence[]
  return RunnableSequence.from<GenerateAuthoritativeAnswerInput, string>([
    RunnablePassthrough.assign({
      context: ({ context }) =>
        context == undefined || context === "" ? "I don't know" : context,
    }),
    answerQuestionPrompt,
    llm,
    new StringOutputParser(),
  ]);
  // end::sequence[]
}

/**
 * How to use this chain in your application:

// tag::usage[]
const llm = new OpenAI() // Or the LLM of your choice
const answerChain = initGenerateAuthoritativeAnswerChain(llm)

const output = await answerChain.invoke({
  input: 'Who is the CEO of Neo4j?',
  context: 'Neo4j CEO: Emil Eifrem',
}) // Emil Eifrem is the CEO of Neo4j
// end::usage[]
 */

================
File: src/solutions/modules/agent/chains/rephrase-question.chain.ts
================
import { StringOutputParser } from "@langchain/core/output_parsers";
import { PromptTemplate } from "@langchain/core/prompts";
import {
  RunnablePassthrough,
  RunnableSequence,
} from "@langchain/core/runnables";

import { BaseChatModel } from "langchain/chat_models/base";
import { ChatbotResponse } from "../history";

// tag::interface[]
export type RephraseQuestionInput = {
  // The user's question
  input: string;
  // Conversation history of {input, output} from the database
  history: ChatbotResponse[];
};
// end::interface[]

// tag::function[]
export default function initRephraseChain(llm: BaseChatModel) {
  // tag::prompt[]
  // Prompt template
  const rephraseQuestionChainPrompt = PromptTemplate.fromTemplate<
    RephraseQuestionInput,
    string
  >(`
    Given the following conversation and a question,
    rephrase the follow-up question to be a standalone question about the
    subject of the conversation history.

    If you do not have the required information required to construct
    a standalone question, ask for clarification.

    Always include the subject of the history in the question.

    History:
    {history}

    Question:
    {input}
  `);
  // end::prompt[]

  // tag::sequence[]
  return RunnableSequence.from<RephraseQuestionInput, string>([
    // <1> Convert message history to a string
    // tag::assign[]
    RunnablePassthrough.assign({
      history: ({ history }): string => {
        if (history.length == 0) {
          return "No history";
        }
        return history
          .map(
            (response: ChatbotResponse) =>
              `Human: ${response.input}\nAI: ${response.output}`
          )
          .join("\n");
      },
    }),
    // end::assign[]
    // <2> Use the input and formatted history to format the prompt
    rephraseQuestionChainPrompt,
    // <3> Pass the formatted prompt to the LLM
    llm,
    // <4> Coerce the output into a string
    new StringOutputParser(),
  ]);
  // end::sequence[]
}
// end::function[]

/**
 * How to use this chain in your application:

// tag::usage[]
const llm = new OpenAI() // Or the LLM of your choice
const rephraseAnswerChain = initRephraseChain(llm)

const output = await rephraseAnswerChain.invoke({
  input: 'What else did they act in?',
  history: [{
    input: 'Who played Woody in Toy Story?',
    output: 'Tom Hanks played Woody in Toy Story',
  }]
}) // Other than Toy Story, what movies has Tom Hanks acted in?
// end::usage[]
 */

================
File: src/solutions/modules/agent/history.ts
================
import { initGraph } from "../../../modules/graph";

type UnpersistedChatbotResponse = {
  input: string;
  rephrasedQuestion: string;
  output: string;
  cypher: string | undefined;
};

export type ChatbotResponse = UnpersistedChatbotResponse & {
  id: string;
};

// tag::clear[]
export async function clearHistory(sessionId: string): Promise<void> {
  const graph = await initGraph();
  await graph.query(
    `
    MATCH (s:Session {id: $sessionId})-[:HAS_RESPONSE]->(r)
    DETACH DELETE r
  `,
    { sessionId },
    "WRITE"
  );
}
// end::clear[]

// tag::get[]
export async function getHistory(
  sessionId: string,
  limit: number = 5
): Promise<ChatbotResponse[]> {
  // tag::gettx[]
  const graph = await initGraph();
  const res = await graph.query<ChatbotResponse>(
    `
      MATCH (:Session {id: $sessionId})-[:LAST_RESPONSE]->(last)
      MATCH path = (start)-[:NEXT*0..${limit}]->(last)
      WHERE length(path) = 5 OR NOT EXISTS { ()-[:NEXT]->(start) }
      UNWIND nodes(path) AS response
      RETURN response.id AS id,
        response.input AS input,
        response.rephrasedQuestion AS rephrasedQuestion,
        response.output AS output,
        response.cypher AS cypher,
        response.createdAt AS createdAt,
        [ (response)-[:CONTEXT]->(n) | elementId(n) ] AS context
    `,
    { sessionId },
    "READ"
  );
  // end::gettx[]

  // tag::getreturn[]
  return res as ChatbotResponse[];
  // end::getreturn[]
}
// end::get[]

// tag::save[]
/**
 * Save a question and response to the database
 *
 * @param {string} sessionId
 * @param {string} source
 * @param {string} input
 * @param {string} rephrasedQuestion
 * @param {string} output
 * @param {string[]} ids
 * @param {string | null} cypher
 * @returns {string}  The ID of the Message node
 */
export async function saveHistory(
  sessionId: string,
  source: string,
  input: string,
  rephrasedQuestion: string,
  output: string,
  ids: string[],
  cypher: string | null = null
): Promise<string> {
  // tag::savetx[]
  const graph = await initGraph();
  const res = await graph.query<{ id: string }>(
    `
    MERGE (session:Session { id: $sessionId }) // <1>

    // <2> Create new response
    CREATE (response:Response {
      id: randomUuid(),
      createdAt: datetime(),
      source: $source,
      input: $input,
      output: $output,
      rephrasedQuestion: $rephrasedQuestion,
      cypher: $cypher,
      ids: $ids
    })
    CREATE (session)-[:HAS_RESPONSE]->(response)

    WITH session, response

    CALL {
    WITH session, response

      // <3> Remove existing :LAST_RESPONSE relationship if it exists
      MATCH (session)-[lrel:LAST_RESPONSE]->(last)
      DELETE lrel

      // <4? Create :NEXT relationship
      CREATE (last)-[:NEXT]->(response)
    }


    // <5> Create new :LAST_RESPONSE relationship
    CREATE (session)-[:LAST_RESPONSE]->(response)

    // <6> Create relationship to context nodes
    WITH response

    CALL {
      WITH response
      UNWIND $ids AS id
      MATCH (context)
      WHERE elementId(context) = id
      CREATE (response)-[:CONTEXT]->(context)

      RETURN count(*) AS count
    }

    RETURN DISTINCT response.id AS id
  `,
    {
      sessionId,
      source,
      input,
      output,
      rephrasedQuestion,
      cypher: cypher,
      ids,
    },
    "WRITE"
  );
  // end::savetx[]

  // tag::savereturn[]
  return res && res.length ? res[0].id : "";
  // end::savereturn[]
}
// end::save[]

================
File: src/solutions/modules/agent/index.ts
================
import { ChatOpenAI } from "@langchain/openai";
import { OpenAIEmbeddings } from "@langchain/openai";
import { Neo4jGraph } from "@langchain/community/graphs/neo4j_graph";
import initAgent from "../../../modules/agent/agent";
import { HumanMessage } from "langchain/schema";
import { initGraph } from "../graph";

// tag::function[]
export async function call(input: string, sessionId: string): Promise<string> {
  // tag::model[]
  const llm = new ChatOpenAI({
    openAIApiKey: process.env.OPENAI_API_KEY,
    // Note: only provide a baseURL when using the GraphAcademy Proxy
    configuration: {
      baseURL: process.env.OPENAI_API_BASE,
    },
  });
  // end::model[]
  // tag::embeddings[]
  const embeddings = new OpenAIEmbeddings({
    openAIApiKey: process.env.OPENAI_API_KEY,
    configuration: {
      baseURL: process.env.OPENAI_API_BASE,
    },
  });
  // end::embeddings[]
  // tag::graph[]
  // Get Graph Singleton
  const graph = await initGraph();
  // end::graph[]

  // tag::call[]
  const agent = await initAgent(llm, embeddings, graph);
  const res = await agent.invoke({ input }, { configurable: { sessionId } });

  return res;
  // end::call[]
}
// end::function[]

================
File: src/solutions/modules/agent/tools/cypher/cypher-evaluation.chain.ts
================
import { BaseLanguageModel } from "langchain/base_language";
import { PromptTemplate } from "@langchain/core/prompts";
import {
  RunnablePassthrough,
  RunnableSequence,
} from "@langchain/core/runnables";
import { JsonOutputParser } from "@langchain/core/output_parsers";

// tag::interface[]
export type CypherEvaluationChainInput = {
  question: string;
  cypher: string;
  schema: string;
  errors: string[] | string | undefined;
};
// end::interface[]

// tag::output[]
export type CypherEvaluationChainOutput = {
  cypher: string;
  errors: string[];
};
// end::output[]

// tag::function[]
export default async function initCypherEvaluationChain(
  llm: BaseLanguageModel
) {
  // tag::prompt[]
  // Prompt template
  const prompt = PromptTemplate.fromTemplate(`
    You are an expert Neo4j Developer evaluating a Cypher statement written by an AI.

    Check that the cypher statement provided below against the database schema to check that
    the statement will answer the user's question.
    Fix any errors where possible.

    The query must:
    * Only use the nodes, relationships and properties mentioned in the schema.
    * Assign a variable to nodes or relationships when intending to access their properties.
    * Use \`IS NOT NULL\` to check for property existence.
    * Use the \`elementId()\` function to return the unique identifier for a node or relationship as \`_id\`.
    * For movies, use the tmdbId property to return a source URL.
      For example: \`'https://www.themoviedb.org/movie/'+ m.tmdbId AS source\`.
    * For movie titles that begin with "The", move "the" to the end.
      For example "The 39 Steps" becomes "39 Steps, The" or "the matrix" becomes "Matrix, The".
    * For the role a person played in a movie, use the role property on the ACTED_IN relationship.
    * Limit the maximum number of results to 10.
    * Respond with only a Cypher statement.  No preamble.

    Respond with a JSON object with "cypher" and "errors" keys.
      * "cypher" - the corrected cypher statement
      * "corrected" - a boolean
      * "errors" - A list of uncorrectable errors.  For example, if a label,
          relationship type or property does not exist in the schema.
          Provide a hint to the correct element where possible.

    Fixable Example #1:
    * cypher:
        MATCH (a:Actor {{name: 'Emil Eifrem'}})-[:ACTED_IN]->(m:Movie)
        RETURN a.name AS Actor, m.title AS Movie, m.tmdbId AS source,
        elementId(m) AS _id, m.released AS ReleaseDate, r.role AS Role LIMIT 10
    * errors: ["Variable \`r\` not defined (line 1, column 172 (offset: 171))"]
    * response:
        MATCH (a:Actor {{\name: 'Emil Eifrem'}})-[r:ACTED_IN]->(m:Movie)
        RETURN a.name AS Actor, m.title AS Movie, m.tmdbId AS source,
        elementId(m) AS _id, m.released AS ReleaseDate, r.role AS Role LIMIT 10


    Schema:
    {schema}

    Question:
    {question}

    Cypher Statement:
    {cypher}

    {errors}
  `);
  // end::prompt[]

  // tag::runnable[]
  // tag::startsequence[]
  return RunnableSequence.from<
    CypherEvaluationChainInput,
    CypherEvaluationChainOutput
  >([
    // end::startsequence[]
    // tag::assign[]
    RunnablePassthrough.assign({
      // Convert errors into an LLM-friendly list
      errors: ({ errors }) => {
        if (
          errors === undefined ||
          (Array.isArray(errors) && errors.length === 0)
        ) {
          return "";
        }

        return `Errors: * ${
          Array.isArray(errors) ? errors?.join("\n* ") : errors
        }`;
      },
    }),
    // end::assign[]
    // tag::rest[]
    prompt,
    llm,
    new JsonOutputParser<CypherEvaluationChainOutput>(),
    // end::rest[]
    // tag::endsequence[]
  ]);
  // end::endsequence[]
  // end::runnable[]
}
// end::function[]

================
File: src/solutions/modules/agent/tools/cypher/cypher-generation.chain.ts
================
import { BaseLanguageModel } from "langchain/base_language";
import { PromptTemplate } from "@langchain/core/prompts";
import {
  RunnablePassthrough,
  RunnableSequence,
} from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers";
import { Neo4jGraph } from "@langchain/community/graphs/neo4j_graph";

// tag::function[]
export default async function initCypherGenerationChain(
  graph: Neo4jGraph,
  llm: BaseLanguageModel
) {
  // tag::prompt[]
  // Create Prompt Template
  const cypherPrompt = PromptTemplate.fromTemplate(`
    You are a Neo4j Developer translating user questions into Cypher to answer questions
    about movies and provide recommendations.
    Convert the user's question into a Cypher statement based on the schema.

    You must:
    * Only use the nodes, relationships and properties mentioned in the schema.
    * When required, \`IS NOT NULL\` to check for property existence, and not the exists() function.
    * Use the \`elementId()\` function to return the unique identifier for a node or relationship as \`_id\`.
      For example:
      \`\`\`
      MATCH (a:Person)-[:ACTED_IN]->(m:Movie)
      WHERE a.name = 'Emil Eifrem'
      RETURN m.title AS title, elementId(m) AS _id, a.role AS role
      \`\`\`
    * Include extra information about the nodes that may help an LLM provide a more informative answer,
      for example the release date, rating or budget.
    * For movies, use the tmdbId property to return a source URL.
      For example: \`'https://www.themoviedb.org/movie/'+ m.tmdbId AS source\`.
    * For movie titles that begin with "The", move "the" to the end.
      For example "The 39 Steps" becomes "39 Steps, The" or "the matrix" becomes "Matrix, The".
    * Limit the maximum number of results to 10.
    * Respond with only a Cypher statement.  No preamble.


    Example Question: What role did Tom Hanks play in Toy Story?
    Example Cypher:
    MATCH (a:Actor {{name: 'Tom Hanks'}})-[rel:ACTED_IN]->(m:Movie {{title: 'Toy Story'}})
    RETURN a.name AS Actor, m.title AS Movie, elementId(m) AS _id, rel.role AS RoleInMovie

    Schema:
    {schema}

    Question:
    {question}
  `);
  // end::prompt[]

  // tag::sequence[]
  // tag::startsequence[]
  // Create the runnable sequence
  return RunnableSequence.from<string, string>([
    // end::startsequence[]
    // tag::assign[]
    {
      // Take the input and assign it to the question key
      question: new RunnablePassthrough(),
      // Get the schema
      schema: () => graph.getSchema(),
    },
    // end::assign[]
    // tag::rest[]
    cypherPrompt,
    llm,
    new StringOutputParser(),
    // end::rest[]
    // tag::endsequence[]
  ]);
  // end::endsequence[]
  // end::sequence[]
}
// end::function[]

================
File: src/solutions/modules/agent/tools/cypher/cypher-retrieval.chain.ts
================
/* eslint-disable indent */
import { BaseLanguageModel } from "langchain/base_language";
import { Neo4jGraph } from "@langchain/community/graphs/neo4j_graph";
import { RunnablePassthrough } from "@langchain/core/runnables";
import initCypherGenerationChain from "./cypher-generation.chain";
import initCypherEvaluationChain from "./cypher-evaluation.chain";
import { saveHistory } from "../../history";
import { AgentToolInput } from "../../../../../modules/agent/agent.types";
import { extractIds } from "../../../../../utils";
import initGenerateAuthoritativeAnswerChain from "../../chains/authoritative-answer-generation.chain";

// tag::input[]
type CypherRetrievalThroughput = AgentToolInput & {
  context: string;
  output: string;
  cypher: string;
  results: Record<string, any> | Record<string, any>[];
  ids: string[];
};
// end::input[]

// tag::recursive[]
/**
 * Use database the schema to generate and subsequently validate
 * a Cypher statement based on the user question
 *
 * @param {Neo4jGraph}        graph     The graph
 * @param {BaseLanguageModel} llm       An LLM to generate the Cypher
 * @param {string}            question  The rephrased question
 * @returns {string}
 */
async function recursivelyEvaluate(
  graph: Neo4jGraph,
  llm: BaseLanguageModel,
  question: string
): Promise<string> {
  // tag::chains[]
  // Initiate chains
  const generationChain = await initCypherGenerationChain(graph, llm);
  const evaluatorChain = await initCypherEvaluationChain(llm);
  // end::chains[]

  // tag::initialcypher[]
  // Generate Initial Cypher
  let cypher = await generationChain.invoke(question);
  // end::initialcypher[]

  // tag::evaluateloop[]
  let errors = ["N/A"];
  let tries = 0;

  while (tries < 5 && errors.length > 0) {
    tries++;

    try {
      // Evaluate Cypher
      const evaluation = await evaluatorChain.invoke({
        question,
        schema: graph.getSchema(),
        cypher,
        errors,
      });

      errors = evaluation.errors;
      cypher = evaluation.cypher;
    } catch (e: unknown) {}
  }
  // end::evaluateloop[]

  // tag::evaluatereturn[]
  // Bug fix: GPT-4 is adamant that it should use id() regardless of
  // the instructions in the prompt.  As a quick fix, replace it here
  cypher = cypher.replace(/\sid\(([^)]+)\)/g, " elementId($1)");

  return cypher;
  // end::evaluatereturn[]
}
// end::recursive[]

// tag::results[]
/**
 * Attempt to get the results, and if there is a syntax error in the Cypher statement,
 * attempt to correct the errors.
 *
 * @param {Neo4jGraph}        graph  The graph instance to get the results from
 * @param {BaseLanguageModel} llm    The LLM to evaluate the Cypher statement if anything goes wrong
 * @param {string}            input  The input built up by the Cypher Retrieval Chain
 * @returns {Promise<Record<string, any>[]>}
 */
export async function getResults(
  graph: Neo4jGraph,
  llm: BaseLanguageModel,
  input: { question: string; cypher: string }
): Promise<any | undefined> {
  // tag::resultvars[]
  let results;
  let retries = 0;
  let cypher = input.cypher;

  // Evaluation chain if an error is thrown by Neo4j
  const evaluationChain = await initCypherEvaluationChain(llm);
  // end::resultvars[]

  // tag::resultloop[]
  while (results === undefined && retries < 5) {
    try {
      results = await graph.query(cypher);
      return results;
    } catch (e: any) {
      retries++;

      const evaluation = await evaluationChain.invoke({
        cypher,
        question: input.question,
        schema: graph.getSchema(),
        errors: [e.message],
      });

      cypher = evaluation.cypher;
    }
  }

  return results;
  // end::resultloop[]
}
// end::results[]

// tag::function[]
export default async function initCypherRetrievalChain(
  llm: BaseLanguageModel,
  graph: Neo4jGraph
) {
  // tag::answerchain[]
  const answerGeneration = await initGenerateAuthoritativeAnswerChain(llm);
  // end::answerchain[]

  // tag::cypher[]
  return (
    RunnablePassthrough
      // Generate and evaluate the Cypher statement
      .assign({
        cypher: (input: { rephrasedQuestion: string }) =>
          recursivelyEvaluate(graph, llm, input.rephrasedQuestion),
      })
      // end::cypher[]

      // tag::getresults[]
      // Get results from database
      .assign({
        results: (input: { cypher: string; question: string }) =>
          getResults(graph, llm, input),
      })
      // end::getresults[]

      // tag::extract[]
      // Extract information
      .assign({
        // Extract _id fields
        ids: (input: Omit<CypherRetrievalThroughput, "ids">) =>
          extractIds(input.results),
        // Convert results to JSON output
        context: ({ results }: Omit<CypherRetrievalThroughput, "ids">) =>
          Array.isArray(results) && results.length == 1
            ? JSON.stringify(results[0])
            : JSON.stringify(results),
      })
      // end::extract[]

      // tag::answer[]
      // Generate Output
      .assign({
        output: (input: CypherRetrievalThroughput) =>
          answerGeneration.invoke({
            question: input.rephrasedQuestion,
            context: input.context,
          }),
      })
      // end::answer[]

      // tag::save[]
      // Save response to database
      .assign({
        responseId: async (input: CypherRetrievalThroughput, options) => {
          saveHistory(
            options?.config.configurable.sessionId,
            "cypher",
            input.input,
            input.rephrasedQuestion,
            input.output,
            input.ids,
            input.cypher
          );
        },
      })
      // end::save[]
      // tag::output[]
      // Return the output
      .pick("output")
  );
  // end::output[]
}
// end::function[]

================
File: src/solutions/modules/agent/tools/index.ts
================
import { BaseChatModel } from "langchain/chat_models/base";
import { Embeddings } from "@langchain/core/embeddings";
import { Neo4jGraph } from "@langchain/community/graphs/neo4j_graph";
import initCypherRetrievalChain from "./cypher/cypher-retrieval.chain";
import initVectorRetrievalChain from "./vector-retrieval.chain";
import { DynamicStructuredTool } from "@langchain/community/tools/dynamic";
import { AgentToolInputSchema } from "../../../../modules/agent/agent.types";

// tag::function[]
export default async function initTools(
  llm: BaseChatModel,
  embeddings: Embeddings,
  graph: Neo4jGraph
): Promise<DynamicStructuredTool[]> {
  // tag::cypherchain[]
  // Initiate chains
  const cypherChain = await initCypherRetrievalChain(llm, graph);
  // end::cypherchain[]
  // tag::retrievalchain[]
  const retrievalChain = await initVectorRetrievalChain(llm, embeddings);
  // end::retrievalchain[]

  return [
    // tag::cypher[]
    new DynamicStructuredTool({
      name: "graph-cypher-retrieval-chain",
      description:
        "For retrieving movie information from the database including movie recommendations, actors and user ratings",
      schema: AgentToolInputSchema,
      func: (input, _runManager, config) => cypherChain.invoke(input, config),
    }),
    // end::cypher[]
    // tag::vector[]
    new DynamicStructuredTool({
      name: "graph-vector-retrieval-chain",
      description:
        "For finding movies, comparing movies by their plot or recommending a movie based on a theme",
      schema: AgentToolInputSchema,
      func: (input, _runManager: any, config) =>
        retrievalChain.invoke(input, config),
    }),
    // end::vector[]
  ];
}
// end::function[]

================
File: src/solutions/modules/agent/tools/vector-retrieval.chain.ts
================
import {
  Runnable,
  RunnablePassthrough,
  RunnablePick,
} from "@langchain/core/runnables";
import { Embeddings } from "@langchain/core/embeddings";
import initGenerateAnswerChain from "../chains/answer-generation.chain";
import { BaseLanguageModel } from "langchain/base_language";
import initVectorStore from "../vector.store";
import { saveHistory } from "../history";
import { DocumentInterface } from "@langchain/core/documents";
import { AgentToolInput } from "../../../../modules/agent/agent.types";

// tag::throughput[]
type RetrievalChainThroughput = AgentToolInput & {
  context: string;
  output: string;
  ids: string[];
};
// end::throughput[]

// tag::extractDocumentIds[]
// Helper function to extract document IDs from Movie node metadata
const extractDocumentIds = (
  documents: DocumentInterface<{ _id: string; [key: string]: any }>[]
): string[] => documents.map((document) => document.metadata._id);
// end::extractDocumentIds[]

// tag::docsToJson[]
// Convert documents to string to be included in the prompt
const docsToJson = (documents: DocumentInterface[]) =>
  JSON.stringify(documents);
// end::docsToJson[]

// tag::function[]
export default async function initVectorRetrievalChain(
  llm: BaseLanguageModel,
  embeddings: Embeddings
): Promise<Runnable<AgentToolInput, string>> {
  // tag::vectorstore[]
  //  Create vector store instance
  const vectorStore = await initVectorStore(embeddings);
  // end::vectorstore[]

  // tag::retriever[]
  // Initialize a retriever wrapper around the vector store
  const vectorStoreRetriever = vectorStore.asRetriever(5);
  // end::retriever[]

  // tag::answerchain[]
  // Initialize  Answer Chain
  const answerChain = initGenerateAnswerChain(llm);
  // end::answerchain[]

  // tag::getcontext[]
  // Get the rephrased question and generate context
  return (
    RunnablePassthrough.assign({
      documents: new RunnablePick("rephrasedQuestion").pipe(
        vectorStoreRetriever
      ),
    })
      // end::getcontext[]
      // tag::mutatecontext[]
      .assign({
        // Extract the IDs
        ids: new RunnablePick("documents").pipe(extractDocumentIds),
        // convert documents to string
        context: new RunnablePick("documents").pipe(docsToJson),
      })
      // end::mutatecontext[]
      // tag::answer[]
      .assign({
        output: (input: RetrievalChainThroughput) =>
          answerChain.invoke({
            question: input.rephrasedQuestion,
            context: input.context,
          }),
      })
      // end::answer[]
      // tag::save[]
      .assign({
        responseId: async (input: RetrievalChainThroughput, options) =>
          saveHistory(
            options?.config.configurable.sessionId,
            "vector",
            input.input,
            input.rephrasedQuestion,
            input.output,
            input.ids
          ),
      })
      // end::save[]
      // tag::output[]
      .pick("output")
  );
  // end::output[]
}
// end::function[]

================
File: src/solutions/modules/agent/vector.store.ts
================
import { EmbeddingsInterface } from "@langchain/core/embeddings";
import { Neo4jVectorStore } from "@langchain/community/vectorstores/neo4j_vector";

/**
 * Create a new vector search index that uses the existing
 * `moviePlots` index.
 *
 * @param {EmbeddingsInterface} embeddings  The embeddings model
 * @returns {Promise<Neo4jVectorStore>}
 */
// tag::function[]
export default async function initVectorStore(
  embeddings: EmbeddingsInterface
): Promise<Neo4jVectorStore> {
  // tag::store[]
  const vectorStore = await Neo4jVectorStore.fromExistingIndex(embeddings, {
    url: process.env.NEO4J_URI as string,
    username: process.env.NEO4J_USERNAME as string,
    password: process.env.NEO4J_PASSWORD as string,
    indexName: "moviePlots",
    textNodeProperty: "plot",
    embeddingNodeProperty: "embedding",
    retrievalQuery: `
      RETURN
        node.plot AS text,
        score,
        {
          _id: elementid(node),
          title: node.title,
          directors: [ (person)-[:DIRECTED]->(node) | person.name ],
          actors: [ (person)-[r:ACTED_IN]->(node) | [person.name, r.role] ],
          tmdbId: node.tmdbId,
          source: 'https://www.themoviedb.org/movie/'+ node.tmdbId
        } AS metadata
    `,
  });
  // end::store[]

  // tag::return[]
  return vectorStore;
  // end::return[]
}
// end::function[]

================
File: src/solutions/modules/graph.ts
================
// tag::import[]
import { Neo4jGraph } from "@langchain/community/graphs/neo4j_graph";
// end::import[]

// tag::graph[]
// <1> The singleton instance
let graph: Neo4jGraph;

/**
 * Return the existing `graph` object or create one
 * has not already been created
 *
 * @returns {Promise<Neo4jGraph>}
 */
export async function initGraph(): Promise<Neo4jGraph> {
  // tag::create[]
  if (!graph) {
    // Create singleton and wait for connection to be verified
    graph = await Neo4jGraph.initialize({
      url: process.env.NEO4J_URI as string,
      username: process.env.NEO4J_USERNAME as string,
      password: process.env.NEO4J_PASSWORD as string,
      database: process.env.NEO4J_DATABASE as string | undefined,
    });
  }
  // end::create[]

  // tag::return[]
  return graph;
  // end::return[]
}
// end::graph[]

================
File: src/utils.ts
================
export function sleep(ms = 500): Promise<void> {
  return new Promise((resolve) => {
    setTimeout(resolve, ms);
  });
}

// tag::extractids[]
export function extractIds(input: any): string[] {
  let output: string[] = [];

  // Function to handle an object
  const handleObject = (item: any) => {
    for (const key in item) {
      if (key === "_id") {
        if (!output.includes(item[key])) {
          output.push(item[key]);
        }
      } else if (typeof item[key] === "object" && item[key] !== null) {
        // Recurse into the object if it is not null
        output = output.concat(extractIds(item[key]));
      }
    }
  };

  if (Array.isArray(input)) {
    // If the input is an array, iterate over each element
    input.forEach((item) => {
      if (typeof item === "object" && item !== null) {
        handleObject(item);
      }
    });
  } else if (typeof input === "object" && input !== null) {
    // If the input is an object, handle it directly
    handleObject(input);
  }

  return output;
}
// end::extractids[]

================
File: tailwind.config.js
================
/** @type {import('tailwindcss').Config} */
module.exports = {
  content: [
    "./app/**/*.{js,ts,jsx,tsx,mdx}",
    "./pages/**/*.{js,ts,jsx,tsx,mdx}",
    "./components/**/*.{js,ts,jsx,tsx,mdx}",

    // Or if using `src` directory:
    "./src/**/*.{js,ts,jsx,tsx,mdx}",
  ],
  plugins: [],
};

================
File: tsconfig.json
================
{
  "compilerOptions": {
    "lib": ["dom", "dom.iterable", "esnext"],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "target": "es2017",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "preserve",
    "incremental": true,
    "plugins": [
      {
        "name": "next"
      }
    ],
    "paths": {
      "@/*": ["./src/*"]
    }
  },
  "include": ["src/**/*, next-env.d.ts", "**/*.ts", "**/*.tsx", ".next/types/**/*.ts"],
  "exclude": ["node_modules", "src/solutions/**/*.ts"]
}
